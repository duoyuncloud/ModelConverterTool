Metadata-Version: 2.4
Name: model-converter-tool
Version: 1.0.0
Summary: A CLI and API tool for converting machine learning models between formats.
Home-page: https://github.com/duoyuncloud/ModelConverterTool
Author-email: Your Name <your@email.com>
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch
Requires-Dist: transformers
Requires-Dist: onnx
Requires-Dist: onnxruntime
Requires-Dist: safetensors
Dynamic: home-page
Dynamic: requires-python

# Model Converter Tool

A powerful, API-first and CLI-friendly tool for converting, validating, and managing machine learning models across multiple formats. Supports real model conversion for ONNX, FP16, HuggingFace, TorchScript, GGUF, MLX, GPTQ, AWQ, and more.

---

## Features

- **Real Model Conversion**: True conversion logic for all supported formats (not dummy models)
- **API First**: Import and use in your own Python code or scripts
- **CLI Friendly**: Full-featured command-line interface for quick operations
- **Multi-format Support**: ONNX, TorchScript, FP16, GPTQ, AWQ, GGUF, MLX, HuggingFace
- **Quantization**: Built-in quantization for GPTQ, AWQ, GGUF (where supported)
- **Batch Conversion**: Convert multiple models via YAML configuration
- **Offline Mode**: Work with local models without internet access
- **Fallback Strategies**: Multi-step fallback for robust conversion
- **Validation**: Validate model files and configurations

---

## Project Structure

```
ModelConverterTool/
├── src/                    # Core conversion logic
│   ├── converter.py       # Main conversion engine
│   ├── config.py          # Configuration management
│   ├── utils.py           # Utility functions
│   └── validator.py       # Validation logic
├── configs/               # YAML configuration files
│   ├── model_presets.yaml # Model presets and configurations
│   └── batch_template.yaml # Batch conversion templates
├── model_converter.py     # CLI entry point
├── requirements.txt       # Dependencies
├── setup.py / pyproject.toml # Packaging
└── README.md
```

---

## Supported Formats

### Input Formats
- Hugging Face models (`hf:model_name`)
- Local model directories
- ONNX models
- TorchScript models
- GGUF models
- MLX models

### Output Formats
- **HuggingFace**: Standard Hugging Face format with optimizations
- **ONNX**: Optimized for inference with dynamic shapes
- **TorchScript**: PyTorch's production-ready format
- **FP16**: Half-precision floating point for memory efficiency
- **GPTQ**: 4-bit quantization for large language models
- **AWQ**: Activation-aware quantization
- **GGUF**: llama.cpp optimized format
- **MLX**: Apple Silicon optimized format

---

## Installation

```bash
# Clone and install
git clone https://github.com/duoyuncloud/ModelConverterTool.git
cd ModelConverterTool
pip install -e .

# Optional: Install quantization dependencies (for GPTQ/AWQ/GGUF/MLX)
pip install auto-gptq awq llama-cpp-python mlx
```

---

## Quick Start

### CLI Usage

```bash
# Convert a HuggingFace model to ONNX
model-converter convert --hf-model gpt2 --output-format onnx --output-path ./outputs/gpt2_onnx

# Convert to FP16
model-converter convert --hf-model gpt2 --output-format fp16 --output-path ./outputs/gpt2_fp16

# Convert with quantization (e.g., GPTQ)
model-converter convert --hf-model distilbert-base-uncased --output-format gptq --output-path ./outputs/distilbert_gptq --quantization q4_k_m

# Batch conversion
model-converter batch-convert --config-file configs/batch_template.yaml

# List supported formats
model-converter list-formats

# Validate a model
model-converter validate --hf-model gpt2 --model-type text-generation
```

### API Usage

```python
from src.converter import ModelConverter

converter = ModelConverter()
success = converter.convert(
    input_source="hf:distilbert-base-uncased",
    output_format="onnx",
    output_path="./outputs/distilbert_onnx",
    model_type="text-classification"
)
print("Conversion success:", success)
```

---

## Configuration

### Model Presets

Predefined model configurations are available in `configs/model_presets.yaml`:

```yaml
common_models:
  bert-base-uncased:
    default_format: onnx
    description: BERT base uncased model
    model_type: text-classification
    supported_formats:
      - onnx
      - gguf
      - mlx
      - torchscript
```

### Custom Configuration

You can create custom YAML configuration files for batch or advanced conversion:

```yaml
model_name: "my-custom-model"
model_type: "text-generation"
output_format: "onnx"
device: "cuda"
quantization: "q4_k_m"
config:
  max_length: 512
  use_cache: false
```

---

## Output Format

All conversions produce outputs that comply with Hugging Face format standards:

```
output_model/
├── model.onnx            # ONNX model file
├── model.pt              # TorchScript model file
├── model.safetensors     # Model weights (HF/FP16)
├── config.json           # Model configuration
├── tokenizer.json        # Tokenizer configuration
├── special_tokens_map.json  # Special tokens
├── format_config.json    # Format-specific metadata
└── README.md             # Model card with conversion info
```

---

## Supported Model Types

- **Text**: text-generation, text-classification, text2text-generation
- **Vision**: image-classification, image-segmentation, object-detection
- **Audio**: audio-classification, audio-ctc, speech-seq2seq
- **Multimodal**: vision-encoder-decoder, question-answering
- **Specialized**: token-classification, multiple-choice, fill-mask

---

## Performance Optimizations

- **Fast Model Loading**: Optimized loading for common models with preset configurations
- **Fallback Strategies**: Multi-step fallback for ONNX, TorchScript, quantization, etc.
- **Memory Efficient**: FP16 and quantized formats for large models

---

## Troubleshooting & FAQ

### Common Issues

1. **ONNX Conversion Fails**
   - Try different model types or simpler models
   - Use `--device cpu` for compatibility
   - Check model complexity and dependencies
2. **Quantization Dependencies**
   - Install required packages: `auto-gptq`, `awq`, `llama-cpp-python`, `mlx`
   - Some quantization requires CUDA support (Linux + NVIDIA GPU)
   - On macOS or without NVIDIA GPU, GPTQ/AWQ real quantization is not available, but other formats work fine
3. **Memory Issues**
   - Use `--device cpu` for large models
   - Try FP16 conversion for memory efficiency
   - Use quantization for very large models

### How to install quantization dependencies?

- These are only required for GPTQ/AWQ/GGUF/MLX formats.
- On macOS or without NVIDIA GPU, you may see installation errors for `auto-gptq`/`awq`—these can be safely ignored unless you need real quantization.
- For real GPTQ/AWQ quantization, use a Linux machine with CUDA and install:
  - [auto-gptq](https://github.com/PanQiWei/AutoGPTQ)
  - [autoawq](https://github.com/casper-hansen/AutoAWQ)

### How to use the API?

- Import `ModelConverter` from `src.converter` and call `convert()` as shown above.
- All CLI features are also available via the API.

---

## License

MIT License

---

## Links

- [GitHub Repository](https://github.com/duoyuncloud/ModelConverterTool)
