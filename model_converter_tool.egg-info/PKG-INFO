Metadata-Version: 2.4
Name: model-converter-tool
Version: 1.0.0
Summary: A CLI and API tool for converting machine learning models between formats.
Home-page: https://github.com/duoyuncloud/ModelConverterTool
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch
Requires-Dist: transformers
Requires-Dist: onnx
Requires-Dist: onnxruntime
Requires-Dist: safetensors
Dynamic: home-page
Dynamic: requires-python

# Model Converter Tool

A CLI and API tool for converting, validating, and managing machine learning models across multiple formats. Supports ONNX, FP16, HuggingFace, TorchScript, GGUF, MLX, GPTQ, AWQ, and more.

---

## Features

- Real model conversion for all supported formats
- API and CLI usage
- Multi-format support: ONNX, TorchScript, FP16, GPTQ, AWQ, GGUF, MLX, HuggingFace
- Quantization for GPTQ, AWQ, GGUF (where supported)
- Batch conversion via YAML configuration
- Offline mode for local models
- Fallback strategies for robust conversion
- Model file and configuration validation

---

## CI and Testing

This project uses GitHub Actions for continuous integration with the following workflows:

### Quick Test (`quick-test.yml`)
- Runs on every push and pull request
- Fast execution (10-minute timeout)
- Tests basic functionality without network access
- Includes linting checks (black, flake8)
- Uses minimal dependencies for speed

### Full Test (`python-tests.yml`)
- Runs on every push and pull request
- Comprehensive testing with multiple Python versions (3.9, 3.10)
- Includes network-dependent tests (model downloads)
- Code coverage reporting
- Full dependency installation

### Test Categories
- **Offline tests**: Basic functionality, validation, error handling
- **Network tests**: Model conversion with real HuggingFace models
- **Integration tests**: End-to-end conversion workflows

### Running Tests Locally

```bash
# Install test dependencies
pip install -r requirements-ci.txt

# Run all tests
pytest tests/ -v

# Run only offline tests (faster)
pytest tests/ -v -m "not network"

# Run only network tests
pytest tests/ -v -m "network"

# Run with coverage
pytest tests/ -v --cov=model_converter_tool --cov-report=html
```

### Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass locally
6. Submit a pull request

The CI will automatically run tests and linting checks on your PR.

---

## Validation

- The `validate` command supports smart auto-detection of model format (ONNX, GGUF, MLX, TorchScript, HuggingFace, etc.) when using `--model-type auto` (default).
- Validation outputs detailed reasoning about format detection and required files.
- No need to manually specify format for most common cases; the tool will infer and validate accordingly.
- Example:

```bash
python3 model_converter.py validate --local-path ./outputs/my_model_dir
```

---

## Project Structure

```
ModelConverterTool/
├── model_converter_tool/      # Core logic
│   ├── converter.py          # Conversion engine
│   ├── config.py             # Configuration management
│   ├── utils.py              # Utility functions
│   └── validator.py          # Validation logic
├── configs/                  # YAML configuration files
│   ├── model_presets.yaml    # Model presets
│   └── batch_template.yaml   # Batch conversion templates
├── .github/workflows/        # CI/CD workflows
│   ├── python-tests.yml      # Full test suite
│   └── quick-test.yml        # Quick tests
├── tests/                    # Test suite
│   ├── test_basic.py         # Core functionality tests
│   ├── test_utils.py         # Utility function tests
│   └── test_validator.py     # Validation tests
├── model_converter.py        # CLI entry point
├── requirements.txt          # Full dependencies
├── requirements-ci.txt       # CI-specific dependencies
├── setup.py / pyproject.toml # Packaging
└── README.md
```

---

## Supported Formats

### Input Formats
- Hugging Face models (`hf:model_name`)
- Local model directories
- ONNX models
- TorchScript models
- GGUF models
- MLX models

### Output Formats
- HuggingFace
- ONNX
- TorchScript
- FP16
- GPTQ
- AWQ
- GGUF
- MLX

---

## Output Directory Structure

After conversion, your output directory will typically look like this:

```
outputs/my_model_onnx/
├── model.onnx            # ONNX model file
├── config.json           # Model configuration
├── tokenizer.json        # Tokenizer config (if applicable)
├── special_tokens_map.json
├── format_config.json    # Format-specific metadata
└── README.md             # Model card with conversion info
```

- Not all files are present for every format. For example, ONNX only requires `model.onnx` and optionally config files.
- The directory name and structure may vary depending on your output path and format.

---

## Version Compatibility

For best results, we recommend the following versions:

- Python: 3.8–3.11
- torch: 2.0 or newer
- transformers: 4.30 or newer
- onnx: 1.13 or newer
- onnxruntime: 1.14 or newer

Other versions may work, but these are tested and supported. If you encounter issues, please check your package versions first.

---

## Installation

```bash
# Clone and install
git clone https://github.com/duoyuncloud/ModelConverterTool.git
cd ModelConverterTool
pip install -e .

# Optional: Install quantization dependencies
pip install auto-gptq awq llama-cpp-python mlx
```

---

## Recommended & Optional Dependencies

The following packages are recommended for advanced features and postprocess options:

| Package         | Purpose                                      | Required for                |
|----------------|----------------------------------------------|-----------------------------|
| onnxsim        | ONNX model simplification                    | postprocess: simplify (ONNX)|
| torch-optimizer| TorchScript optimization                     | postprocess: optimize (TS)  |
| safetensors    | Fast model saving/loading                    | FP16, general use           |

To install all optional features:

```bash
pip install onnxsim torch-optimizer
```

If you only need basic conversion, these are not required.

---

## Typical User Workflow

Most users use this tool to convert their own locally trained models (e.g., HuggingFace, PyTorch, ONNX) to other formats for deployment or interoperability. You can convert a single model or batch convert multiple models using a YAML configuration file. Public model pre-download is not required for normal use.

---

## Quick Start

### CLI Usage

```bash
model-converter convert --hf-model gpt2 --output-format onnx --output-path ./outputs/gpt2_onnx
model-converter convert --hf-model gpt2 --output-format fp16 --output-path ./outputs/gpt2_fp16
model-converter convert --hf-model distilbert-base-uncased --output-format gptq --output-path ./outputs/distilbert_gptq --quantization q4_k_m
model-converter batch-convert --config-file configs/batch_template.yaml
model-converter list-formats
model-converter validate --hf-model gpt2 --model-type text-generation
```

### API Usage

```python
from model_converter_tool import ModelConverter

converter = ModelConverter()
success = converter.convert(
    input_source="hf:distilbert-base-uncased",
    output_format="onnx",
    output_path="./outputs/distilbert_onnx",
    model_type="text-classification"
)
print("Conversion success:", success)
```

---

## Batch Conversion & Configuration

You can convert multiple models at once, or use advanced options, by editing a YAML configuration file.

- Each model in the YAML can have its own `device`, `quantization`, and `postprocess` fields (all optional).
- After each conversion, validation is automatically run and logs are saved to `outputs/logs/{model_name}.log`.

### 1. Copy the template

```bash
cp configs/batch_template.yaml configs/my_batch.yaml
```

### 2. Edit your YAML config

Example for batch conversion:

```yaml
models:
  my_model_1:
    input: "/path/to/my_model_1"
    output_format: "onnx"
    output_path: "outputs/my_model_1_onnx"
    model_type: "text-classification"
    device: "cpu"           # optional
    quantization: "q4_k_m"  # optional
    postprocess: "strip_unused_nodes" # optional
  my_model_2:
    input: "/path/to/my_model_2"
    output_format: "torchscript"
    output_path: "outputs/my_model_2_ts"
    model_type: "text-generation"
    device: "cuda"
```

**Field explanations:**
- `device`: Which device to use for conversion. Options: `"cpu"`, `"cuda"`, or `"auto"` (default, auto-selects GPU if available).
- `quantization`: Quantization method to reduce model size or speed up inference. Options include `"fp16"`, `"int8"`, `"q4_k_m"`, etc. Not all models/formats support all methods.
- `postprocess`: Extra processing after conversion. For ONNX models, you can use `"simplify"` (requires onnx-simplifier) or `"optimize"` (requires onnxoptimizer). For TorchScript models, you can use `"optimize"` (uses torch.jit.optimize_for_inference). For FP16 models, you can use `"prune"` (sets small weights to zero, requires safetensors). GGUF and MLX postprocess are reserved for future use.

You can also use a YAML file for advanced single-model conversion.

You can place your YAML config anywhere, but we recommend keeping it in the `configs/` directory for organization.

### 3. Run batch conversion

```bash
python3 model_converter.py batch-convert --config-file configs/my_batch.yaml
```

---

## Offline Mode

You can run the converter in **offline mode** to ensure no downloads are performed and only local model files are used. This is useful for air-gapped, reproducible, or secure environments.

- To enable offline mode, add the `--offline-mode` flag to your CLI command.
- In offline mode, you must provide a local model path (not a HuggingFace model name).
- If you try to use a HuggingFace model (e.g., `--hf-model gpt2`) in offline mode, the tool will show an error and stop.

### CLI Example

```bash
# Convert a local model in offline mode
model-converter convert --local-path /models/my_model --output-format onnx --offline-mode

# Batch convert with offline mode
model-converter batch-convert --config-file configs/batch_template.yaml --offline-mode
```

### API Example

```python
converter.convert(
    input_source="/models/my_model",
    output_format="onnx",
    output_path="outputs/my_model_onnx",
    offline_mode=True
)
```

---

## Supported Model Types

- Text: text-generation, text-classification, text2text-generation
- Vision: image-classification, image-segmentation, object-detection
- Audio: audio-classification, audio-ctc, speech-seq2seq
- Multimodal: vision-encoder-decoder, question-answering
- Specialized: token-classification, multiple-choice, fill-mask

---

## Troubleshooting & FAQ

### Common Issues

1. ONNX Conversion Fails
   - Try different model types or simpler models
   - Use `--device cpu` for compatibility
   - Check model complexity and dependencies
2. Quantization Dependencies
   - Install required packages: `auto-gptq`, `awq`, `llama-cpp-python`, `mlx`
   - Some quantization requires CUDA support (Linux + NVIDIA GPU)

### Frequently Asked Questions

**Q: Do I need to pre-download public models?**  
A: No, unless you want to test or demo. For your own models, just use `--local-path` and point to your local model directory.

**Q: Where should I put my YAML config?**  
A: Anywhere you like! The `configs/` directory is recommended for organization, but you can specify any path with `--config-file`.

**Q: Can I convert models fully offline?**  
A: Yes! Use `--offline-mode` and provide a local model path. No internet connection is required for local models.
