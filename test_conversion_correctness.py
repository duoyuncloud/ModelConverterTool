#!/usr/bin/env python3
"""
Deep test to verify the correctness of GPT-2 conversions by comparing weights and outputs
"""

import tempfile
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
import torch

from model_converter_tool.converter import ModelConverter


def test_conversion_correctness():
    """Test the correctness of GPT-2 conversions by comparing with original model"""
    
    print("ğŸ”¬ GPT-2 è½¬æ¢æ­£ç¡®æ€§æ·±åº¦éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    converter = ModelConverter()
    
    # åŠ è½½åŸå§‹GPT-2æ¨¡å‹ä½œä¸ºåŸºå‡†
    print("ğŸ“¥ åŠ è½½åŸå§‹GPT-2æ¨¡å‹ä½œä¸ºåŸºå‡†...")
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    original_model = AutoModelForCausalLM.from_pretrained('gpt2')
    original_tokenizer = AutoTokenizer.from_pretrained('gpt2')
    
    # è·å–åŸå§‹æ¨¡å‹çš„æƒé‡ä¿¡æ¯
    original_weights = {}
    for name, param in original_model.named_parameters():
        original_weights[name] = {
            'shape': tuple(param.shape),
            'dtype': str(param.dtype),
            'mean': float(param.mean().item()),
            'std': float(param.std().item()),
            'min': float(param.min().item()),
            'max': float(param.max().item())
        }
    
    print(f"åŸå§‹æ¨¡å‹å‚æ•°æ•°é‡: {len(original_weights)}")
    print(f"åŸå§‹æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in original_model.parameters()):,}")
    
    # æµ‹è¯•è¾“å…¥
    test_text = "Hello world"
    test_inputs = original_tokenizer(test_text, return_tensors='pt')
    
    # è·å–åŸå§‹æ¨¡å‹çš„è¾“å‡º
    with torch.no_grad():
        original_outputs = original_model(**test_inputs)
    original_logits = original_outputs.logits
    
    print(f"åŸå§‹æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {original_logits.shape}")
    print(f"åŸå§‹æ¨¡å‹è¾“å‡ºç»Ÿè®¡: mean={original_logits.mean().item():.6f}, std={original_logits.std().item():.6f}")
    
    # æµ‹è¯•æ ¼å¼åˆ—è¡¨
    test_formats = [
        ('fp16', 'FP16 åŠç²¾åº¦'),
        ('hf', 'Hugging Face'),
    ]
    
    results = {}
    
    for format_type, format_name in test_formats:
        print(f"\nğŸ“‹ æµ‹è¯• {format_name} è½¬æ¢æ­£ç¡®æ€§")
        print("-" * 50)
        
        with tempfile.TemporaryDirectory() as temp_dir:
            out_dir = Path(temp_dir) / f'gpt2_{format_type}'
            out_dir.mkdir()
            
            # æ‰§è¡Œè½¬æ¢
            result = converter.convert(
                input_source='hf:gpt2',
                output_format=format_type,
                output_path=str(out_dir),
                model_type='text-generation',
                validate=False  # æˆ‘ä»¬è‡ªå·±éªŒè¯
            )
            
            if result['success']:
                print(f"è½¬æ¢æˆåŠŸ: âœ…")
                
                # åŠ è½½è½¬æ¢åçš„æ¨¡å‹
                try:
                    converted_model = AutoModelForCausalLM.from_pretrained(str(out_dir))
                    converted_tokenizer = AutoTokenizer.from_pretrained(str(out_dir))
                    
                    # è·å–è½¬æ¢åæ¨¡å‹çš„æƒé‡ä¿¡æ¯
                    converted_weights = {}
                    for name, param in converted_model.named_parameters():
                        converted_weights[name] = {
                            'shape': tuple(param.shape),
                            'dtype': str(param.dtype),
                            'mean': float(param.mean().item()),
                            'std': float(param.std().item()),
                            'min': float(param.min().item()),
                            'max': float(param.max().item())
                        }
                    
                    # æƒé‡æ¯”è¾ƒåˆ†æ
                    weight_analysis = compare_weights(original_weights, converted_weights)
                    
                    # è¾“å‡ºæ¯”è¾ƒ
                    with torch.no_grad():
                        converted_outputs = converted_model(**test_inputs)
                    converted_logits = converted_outputs.logits
                    
                    output_analysis = compare_outputs(original_logits, converted_logits)
                    
                    # æ¨ç†æµ‹è¯•
                    inference_analysis = test_inference_correctness(
                        original_model, original_tokenizer,
                        converted_model, converted_tokenizer
                    )
                    
                    # ä¿å­˜ç»“æœ
                    results[format_type] = {
                        'success': True,
                        'weight_analysis': weight_analysis,
                        'output_analysis': output_analysis,
                        'inference_analysis': inference_analysis,
                        'model_info': {
                            'original_params': len(original_weights),
                            'converted_params': len(converted_weights),
                            'original_total_params': sum(p.numel() for p in original_model.parameters()),
                            'converted_total_params': sum(p.numel() for p in converted_model.parameters())
                        }
                    }
                    
                    print(f"æƒé‡æ¯”è¾ƒ: {weight_analysis['summary']}")
                    print(f"è¾“å‡ºæ¯”è¾ƒ: {output_analysis['summary']}")
                    print(f"æ¨ç†æµ‹è¯•: {inference_analysis['summary']}")
                    
                except Exception as e:
                    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    results[format_type] = {
                        'success': False,
                        'error': str(e)
                    }
            else:
                print(f"è½¬æ¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                results[format_type] = {
                    'success': False,
                    'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                }
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š è½¬æ¢æ­£ç¡®æ€§æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    for format_type, format_name in test_formats:
        result = results[format_type]
        if result['success']:
            print(f"\nâœ… {format_name}:")
            
            weight_analysis = result['weight_analysis']
            output_analysis = result['output_analysis']
            inference_analysis = result['inference_analysis']
            
            print(f"  æƒé‡æ­£ç¡®æ€§: {weight_analysis['correctness_score']:.1f}/10")
            print(f"  è¾“å‡ºæ­£ç¡®æ€§: {output_analysis['correctness_score']:.1f}/10")
            print(f"  æ¨ç†æ­£ç¡®æ€§: {inference_analysis['correctness_score']:.1f}/10")
            
            # è®¡ç®—æ€»ä½“æ­£ç¡®æ€§è¯„åˆ†
            overall_score = (
                weight_analysis['correctness_score'] * 0.4 +
                output_analysis['correctness_score'] * 0.4 +
                inference_analysis['correctness_score'] * 0.2
            )
            print(f"  æ€»ä½“æ­£ç¡®æ€§: {overall_score:.1f}/10")
            
            # è¯¦ç»†åˆ†æ
            if weight_analysis['shape_mismatches']:
                print(f"    å½¢çŠ¶ä¸åŒ¹é…: {len(weight_analysis['shape_mismatches'])} ä¸ªå‚æ•°")
            if weight_analysis['dtype_changes']:
                print(f"    æ•°æ®ç±»å‹å˜åŒ–: {len(weight_analysis['dtype_changes'])} ä¸ªå‚æ•°")
            
            print(f"    è¾“å‡ºå·®å¼‚: {output_analysis['mean_diff']:.6f}")
            print(f"    æ¨ç†ä¸€è‡´æ€§: {inference_analysis['consistency_score']:.1%}")
            
        else:
            print(f"\nâŒ {format_name}: è½¬æ¢å¤±è´¥")
            print(f"  é”™è¯¯: {result['error']}")
    
    return results


def compare_weights(original_weights: Dict, converted_weights: Dict) -> Dict[str, Any]:
    """æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œè½¬æ¢åæ¨¡å‹çš„æƒé‡"""
    
    shape_mismatches = []
    dtype_changes = []
    value_differences = []
    
    # æ£€æŸ¥æ‰€æœ‰å‚æ•°
    for name in original_weights:
        if name not in converted_weights:
            shape_mismatches.append(name)
            continue
            
        orig = original_weights[name]
        conv = converted_weights[name]
        
        # æ£€æŸ¥å½¢çŠ¶
        if orig['shape'] != conv['shape']:
            shape_mismatches.append(name)
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        if orig['dtype'] != conv['dtype']:
            dtype_changes.append(name)
        
        # æ£€æŸ¥æ•°å€¼å·®å¼‚ï¼ˆå¯¹äºç›¸åŒå½¢çŠ¶çš„å‚æ•°ï¼‰
        if orig['shape'] == conv['shape']:
            mean_diff = abs(orig['mean'] - conv['mean'])
            std_diff = abs(orig['std'] - conv['std'])
            value_differences.append({
                'name': name,
                'mean_diff': mean_diff,
                'std_diff': std_diff
            })
    
    # è®¡ç®—æ­£ç¡®æ€§è¯„åˆ†
    total_params = len(original_weights)
    shape_correct = total_params - len(shape_mismatches)
    dtype_correct = total_params - len(dtype_changes)
    
    # è®¡ç®—æ•°å€¼å·®å¼‚çš„å¹³å‡å€¼
    if value_differences:
        avg_mean_diff = sum(d['mean_diff'] for d in value_differences) / len(value_differences)
        avg_std_diff = sum(d['std_diff'] for d in value_differences) / len(value_differences)
    else:
        avg_mean_diff = 0
        avg_std_diff = 0
    
    # è¯„åˆ†è®¡ç®—
    shape_score = (shape_correct / total_params) * 4  # 40%æƒé‡
    dtype_score = (dtype_correct / total_params) * 2   # 20%æƒé‡
    value_score = max(0, 4 - avg_mean_diff * 100)      # 40%æƒé‡ï¼ŒåŸºäºæ•°å€¼å·®å¼‚
    
    correctness_score = min(10, shape_score + dtype_score + value_score)
    
    return {
        'shape_mismatches': shape_mismatches,
        'dtype_changes': dtype_changes,
        'value_differences': value_differences,
        'avg_mean_diff': avg_mean_diff,
        'avg_std_diff': avg_std_diff,
        'correctness_score': correctness_score,
        'summary': f"å½¢çŠ¶åŒ¹é…: {shape_correct}/{total_params}, æ•°å€¼å·®å¼‚: {avg_mean_diff:.6f}"
    }


def compare_outputs(original_logits: torch.Tensor, converted_logits: torch.Tensor) -> Dict[str, Any]:
    """æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œè½¬æ¢åæ¨¡å‹çš„è¾“å‡º"""
    
    # ç¡®ä¿å½¢çŠ¶ç›¸åŒ
    if original_logits.shape != converted_logits.shape:
        return {
            'correctness_score': 0,
            'mean_diff': float('inf'),
            'max_diff': float('inf'),
            'summary': f"å½¢çŠ¶ä¸åŒ¹é…: {original_logits.shape} vs {converted_logits.shape}"
        }
    
    # è®¡ç®—å·®å¼‚
    diff = torch.abs(original_logits - converted_logits)
    mean_diff = float(diff.mean().item())
    max_diff = float(diff.max().item())
    
    # è®¡ç®—æ­£ç¡®æ€§è¯„åˆ†
    # åŸºäºå·®å¼‚å¤§å°è¯„åˆ†ï¼Œå·®å¼‚è¶Šå°åˆ†æ•°è¶Šé«˜
    if mean_diff < 1e-6:
        correctness_score = 10.0
    elif mean_diff < 1e-5:
        correctness_score = 9.0
    elif mean_diff < 1e-4:
        correctness_score = 8.0
    elif mean_diff < 1e-3:
        correctness_score = 7.0
    elif mean_diff < 1e-2:
        correctness_score = 6.0
    elif mean_diff < 1e-1:
        correctness_score = 5.0
    else:
        correctness_score = max(0, 5 - mean_diff)
    
    return {
        'mean_diff': mean_diff,
        'max_diff': max_diff,
        'correctness_score': correctness_score,
        'summary': f"å¹³å‡å·®å¼‚: {mean_diff:.6f}, æœ€å¤§å·®å¼‚: {max_diff:.6f}"
    }


def test_inference_correctness(
    original_model, original_tokenizer,
    converted_model, converted_tokenizer
) -> Dict[str, Any]:
    """æµ‹è¯•æ¨ç†æ­£ç¡®æ€§"""
    
    test_cases = [
        "Hello world",
        "The quick brown fox",
        "Machine learning is",
        "In the beginning",
        "To be or not to be"
    ]
    
    consistent_count = 0
    total_tests = len(test_cases)
    
    for test_text in test_cases:
        try:
            # åŸå§‹æ¨¡å‹æ¨ç†
            orig_inputs = original_tokenizer(test_text, return_tensors='pt')
            with torch.no_grad():
                orig_outputs = original_model(**orig_inputs)
            orig_logits = orig_outputs.logits
            
            # è½¬æ¢åæ¨¡å‹æ¨ç†
            conv_inputs = converted_tokenizer(test_text, return_tensors='pt')
            with torch.no_grad():
                conv_outputs = converted_model(**conv_inputs)
            conv_logits = conv_outputs.logits
            
            # æ¯”è¾ƒè¾“å‡º
            if orig_logits.shape == conv_logits.shape:
                diff = torch.abs(orig_logits - conv_logits).mean().item()
                if diff < 1e-3:  # å…è®¸ä¸€å®šçš„æ•°å€¼è¯¯å·®
                    consistent_count += 1
                    
        except Exception as e:
            print(f"æ¨ç†æµ‹è¯•å¤±è´¥ ({test_text}): {e}")
    
    consistency_score = consistent_count / total_tests
    correctness_score = consistency_score * 10
    
    return {
        'consistent_count': consistent_count,
        'total_tests': total_tests,
        'consistency_score': consistency_score,
        'correctness_score': correctness_score,
        'summary': f"ä¸€è‡´æ€§: {consistent_count}/{total_tests} ({consistency_score:.1%})"
    }


if __name__ == "__main__":
    results = test_conversion_correctness()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('conversion_correctness_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: conversion_correctness_results.json") 