#!/usr/bin/env python3
"""
å¿«é€ŸGPUé‡åŒ–æµ‹è¯•è„šæœ¬
éªŒè¯GPUæœåŠ¡å™¨ä¸‹çš„é‡åŒ–è½¬æ¢æ˜¯å¦çœŸå®æœ‰æ•ˆ
"""

import os
import time
import torch
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œé‡åŒ–ï¼ˆé¿å…MPSå…¼å®¹æ€§é—®é¢˜ï¼‰
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["USE_CPU_ONLY"] = "1"

def check_gpu():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    logger.info("=== GPUçŠ¶æ€æ£€æŸ¥ ===")
    
    cuda_available = torch.cuda.is_available()
    mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    if cuda_available:
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… CUDAå¯ç”¨: {device_name}")
        logger.info(f"   GPUå†…å­˜: {memory_gb:.2f} GB")
        return "cuda"
    elif mps_available:
        logger.info("âœ… MPSå¯ç”¨ï¼ˆä½†é‡åŒ–æµ‹è¯•å°†ä½¿ç”¨CPUï¼‰")
        return "mps"
    else:
        logger.info("âŒ ä»…CPUå¯ç”¨")
        return "cpu"

def test_gpu_quantization():
    """æµ‹è¯•GPUé‡åŒ–"""
    logger.info("=== é‡åŒ–æµ‹è¯•ï¼ˆä½¿ç”¨CPUé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰ ===")
    
    # å¯¼å…¥è½¬æ¢å™¨
    from model_converter_tool.converter import ModelConverter
    converter = ModelConverter()
    
    # æµ‹è¯•æ¨¡å‹
    test_model = "facebook/opt-125m"
    output_dir = Path("test_outputs/quick_gpu_test")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æµ‹è¯•GPTQé‡åŒ–
    logger.info("æµ‹è¯•GPTQé‡åŒ–...")
    start_time = time.time()
    
    result = converter.convert(
        input_source=test_model,
        output_format="gptq",
        output_path=str(output_dir / "gptq_test"),
        model_type="text-generation",
        device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
        validate=True
    )
    
    duration = time.time() - start_time
    
    if result["success"]:
        logger.info(f"âœ… GPTQé‡åŒ–æˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        output_path = output_dir / "gptq_test"
        if output_path.exists():
            files = [f.name for f in output_path.iterdir() if f.is_file()]
            logger.info(f"   è¾“å‡ºæ–‡ä»¶: {files}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡åŒ–æ–‡ä»¶
            has_quantized_files = any(f.endswith(('.safetensors', '.bin')) for f in files)
            if has_quantized_files:
                logger.info("âœ… å‘ç°é‡åŒ–æ¨¡å‹æ–‡ä»¶")
            else:
                logger.warning("âš ï¸ æœªå‘ç°é‡åŒ–æ¨¡å‹æ–‡ä»¶")
        else:
            logger.error("âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
    else:
        logger.error(f"âŒ GPTQé‡åŒ–å¤±è´¥: {result.get('error')}")
    
    return result["success"], duration

def test_awq_quantization():
    """æµ‹è¯•AWQé‡åŒ–"""
    logger.info("=== AWQé‡åŒ–æµ‹è¯• ===")
    
    from model_converter_tool.converter import ModelConverter
    converter = ModelConverter()
    
    test_model = "facebook/opt-125m"
    output_dir = Path("test_outputs/quick_gpu_test")
    
    logger.info("æµ‹è¯•AWQé‡åŒ–...")
    start_time = time.time()
    
    result = converter.convert(
        input_source=test_model,
        output_format="awq",
        output_path=str(output_dir / "awq_test"),
        model_type="text-generation",
        device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
        validate=True
    )
    
    duration = time.time() - start_time
    
    if result["success"]:
        logger.info(f"âœ… AWQé‡åŒ–æˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’")
        
        output_path = output_dir / "awq_test"
        if output_path.exists():
            files = [f.name for f in output_path.iterdir() if f.is_file()]
            logger.info(f"   è¾“å‡ºæ–‡ä»¶: {files}")
    else:
        logger.error(f"âŒ AWQé‡åŒ–å¤±è´¥: {result.get('error')}")
    
    return result["success"], duration

def test_gguf_quantization():
    """æµ‹è¯•GGUFé‡åŒ–"""
    logger.info("=== GGUFé‡åŒ–æµ‹è¯• ===")
    
    from model_converter_tool.converter import ModelConverter
    converter = ModelConverter()
    
    test_model = "facebook/opt-125m"
    output_dir = Path("test_outputs/quick_gpu_test")
    
    quantization_levels = ["q4_k_m", "q8_0"]
    results = {}
    
    for quant_level in quantization_levels:
        logger.info(f"æµ‹è¯•GGUFé‡åŒ–çº§åˆ«: {quant_level}")
        start_time = time.time()
        
        result = converter.convert(
            input_source=test_model,
            output_format="gguf",
            output_path=str(output_dir / f"gguf_{quant_level}.gguf"),
            model_type="text-generation",
            quantization=quant_level,
            device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
            validate=True
        )
        
        duration = time.time() - start_time
        
        if result["success"]:
            # æ£€æŸ¥GGUFæ–‡ä»¶
            gguf_dir = output_dir / f"gguf_{quant_level}.gguf"
            gguf_file = None
            
            # æŸ¥æ‰¾.ggufæ–‡ä»¶
            for file_path in gguf_dir.iterdir():
                if file_path.is_file() and file_path.suffix == '.gguf':
                    gguf_file = file_path
                    break
            
            if gguf_file and gguf_file.exists():
                file_size_mb = gguf_file.stat().st_size / 1024**2
                logger.info(f"âœ… GGUFé‡åŒ– {quant_level} æˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’ï¼Œæ–‡ä»¶å¤§å°: {file_size_mb:.2f}MB")
                results[quant_level] = {"success": True, "duration": duration, "file_size_mb": file_size_mb}
            else:
                logger.error(f"âŒ GGUFé‡åŒ– {quant_level} æˆåŠŸä½†è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                results[quant_level] = {"success": False, "error": "è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨"}
        else:
            logger.error(f"âŒ GGUFé‡åŒ– {quant_level} å¤±è´¥: {result.get('error')}")
            results[quant_level] = {"success": False, "error": result.get('error')}
    
    return results

def test_cpu_vs_gpu():
    """å¯¹æ¯”CPUå’ŒGPUæ€§èƒ½"""
    logger.info("=== CPU vs GPUæ€§èƒ½å¯¹æ¯” ===")
    
    from model_converter_tool.converter import ModelConverter
    converter = ModelConverter()
    
    test_model = "facebook/opt-125m"
    output_dir = Path("test_outputs/quick_gpu_test")
    
    # CPUæµ‹è¯•
    logger.info("CPUé‡åŒ–æµ‹è¯•...")
    start_time = time.time()
    
    cpu_result = converter.convert(
        input_source=test_model,
        output_format="gptq",
        output_path=str(output_dir / "cpu_test"),
        model_type="text-generation",
        device="cpu",
        validate=False
    )
    cpu_time = time.time() - start_time
    
    # GPUæµ‹è¯•ï¼ˆå¦‚æœæœ‰CUDAï¼‰
    if torch.cuda.is_available():
        logger.info("GPUé‡åŒ–æµ‹è¯•...")
        start_time = time.time()
        
        gpu_result = converter.convert(
            input_source=test_model,
            output_format="gptq",
            output_path=str(output_dir / "gpu_test"),
            model_type="text-generation",
            device="cuda",
            validate=False
        )
        gpu_time = time.time() - start_time
        
        if cpu_result["success"] and gpu_result["success"]:
            speedup = cpu_time / gpu_time
            logger.info(f"CPUè€—æ—¶: {cpu_time:.2f}ç§’")
            logger.info(f"GPUè€—æ—¶: {gpu_time:.2f}ç§’")
            logger.info(f"GPUåŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1.0:
                logger.info("âœ… GPUç¡®å®æä¾›äº†åŠ é€Ÿ")
            else:
                logger.warning("âš ï¸ GPUæœªæä¾›åŠ é€Ÿ")
        else:
            logger.error("âŒ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥")
            speedup = None
    else:
        logger.info("æ— CUDA GPUï¼Œè·³è¿‡GPUæ€§èƒ½å¯¹æ¯”")
        speedup = None
    
    return cpu_result["success"], speedup, cpu_time

def test_quantization_effectiveness():
    """æµ‹è¯•é‡åŒ–æ•ˆæœ"""
    logger.info("=== é‡åŒ–æ•ˆæœæµ‹è¯• ===")
    
    output_dir = Path("test_outputs/quick_gpu_test")
    
    # æ£€æŸ¥é‡åŒ–æ¨¡å‹æ–‡ä»¶å¤§å°
    gptq_path = output_dir / "gptq_test"
    if gptq_path.exists():
        total_size = 0
        file_count = 0
        
        for file_path in gptq_path.iterdir():
            if file_path.is_file():
                size_mb = file_path.stat().st_size / 1024**2
                total_size += size_mb
                file_count += 1
                logger.info(f"æ–‡ä»¶: {file_path.name}, å¤§å°: {size_mb:.2f}MB")
        
        logger.info(f"é‡åŒ–æ¨¡å‹æ€»å¤§å°: {total_size:.2f}MB ({file_count}ä¸ªæ–‡ä»¶)")
        
        # ä¼°ç®—åŸå§‹æ¨¡å‹å¤§å°ï¼ˆOPT-125Må¤§çº¦500MBï¼‰
        original_size = 500  # MB
        compression_ratio = original_size / total_size if total_size > 0 else 0
        
        logger.info(f"å‹ç¼©æ¯”: {compression_ratio:.2f}x")
        
        if compression_ratio > 1.5:
            logger.info("âœ… é‡åŒ–å‹ç¼©æ•ˆæœæ˜æ˜¾")
        else:
            logger.warning("âš ï¸ é‡åŒ–å‹ç¼©æ•ˆæœä¸æ˜æ˜¾")
    else:
        logger.error("âŒ é‡åŒ–æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹å¿«é€Ÿé‡åŒ–æµ‹è¯•")
    
    # 1. æ£€æŸ¥GPU
    device = check_gpu()
    
    # 2. æµ‹è¯•GPTQé‡åŒ–
    gptq_success, gptq_duration = test_gpu_quantization()
    
    # 3. æµ‹è¯•AWQé‡åŒ–
    awq_success, awq_duration = test_awq_quantization()
    
    # 4. æµ‹è¯•GGUFé‡åŒ–
    gguf_results = test_gguf_quantization()
    
    # 5. æ€§èƒ½å¯¹æ¯”
    cpu_success, speedup, cpu_time = test_cpu_vs_gpu()
    
    # 6. é‡åŒ–æ•ˆæœ
    test_quantization_effectiveness()
    
    # æ€»ç»“
    logger.info("=== æµ‹è¯•æ€»ç»“ ===")
    logger.info(f"è®¾å¤‡: {device}")
    logger.info(f"GPTQé‡åŒ–: {'âœ… æˆåŠŸ' if gptq_success else 'âŒ å¤±è´¥'}")
    if gptq_success:
        logger.info(f"  - è€—æ—¶: {gptq_duration:.2f}ç§’")
    
    logger.info(f"AWQé‡åŒ–: {'âœ… æˆåŠŸ' if awq_success else 'âŒ å¤±è´¥'}")
    if awq_success:
        logger.info(f"  - è€—æ—¶: {awq_duration:.2f}ç§’")
    
    logger.info("GGUFé‡åŒ–ç»“æœ:")
    for level, result in gguf_results.items():
        status = "âœ… æˆåŠŸ" if result.get('success') else "âŒ å¤±è´¥"
        logger.info(f"  - {level}: {status}")
        if result.get('success'):
            logger.info(f"    - è€—æ—¶: {result['duration']:.2f}ç§’")
            if 'file_size_mb' in result:
                logger.info(f"    - æ–‡ä»¶å¤§å°: {result['file_size_mb']:.2f}MB")
    
    if speedup:
        logger.info(f"GPUåŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup > 1.0:
            logger.info("ğŸ‰ GPUé‡åŒ–è½¬æ¢çœŸå®æœ‰æ•ˆï¼")
        else:
            logger.info("âš ï¸ GPUé‡åŒ–è½¬æ¢æ•ˆæœä¸æ˜æ˜¾")
    else:
        logger.info("â„¹ï¸ æ— æ³•è¿›è¡ŒGPUæ€§èƒ½å¯¹æ¯”")
    
    # æ€»ä½“æˆåŠŸç‡
    total_tests = 1 + 1 + len(gguf_results)  # GPTQ + AWQ + GGUF
    successful_tests = sum([gptq_success, awq_success] + [r.get('success', False) for r in gguf_results.values()])
    success_rate = successful_tests / total_tests * 100
    
    logger.info(f"æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}% ({successful_tests}/{total_tests})")

if __name__ == "__main__":
    main() 