============================= test session starts ==============================
platform darwin -- Python 3.13.5, pytest-8.4.1, pluggy-1.6.0 -- /Users/duoyun/Desktop/ModelConverterTool/venv/bin/python3.13
cachedir: .pytest_cache
rootdir: /Users/duoyun/Desktop/ModelConverterTool
configfile: pyproject.toml
collecting ... collected 7 items

tests/test_basic_conversions.py::test_readme_demo[task0] PASSED          [ 14%]
tests/test_basic_conversions.py::test_readme_demo[task1] SKIPPED (Model
Qwen/Qwen1.5-0.5B-Chat not available on HuggingFace)                     [ 28%]
tests/test_basic_conversions.py::test_readme_demo[task2] FAILED          [ 42%]
tests/test_basic_conversions.py::test_readme_demo[task3] PASSED          [ 57%]
tests/test_basic_conversions.py::test_readme_demo[task4] PASSED          [ 71%]
tests/test_basic_conversions.py::test_readme_demo[task5] PASSED          [ 85%]
tests/test_basic_conversions.py::test_readme_demo[task6] PASSED          [100%]

=================================== FAILURES ===================================
___________________________ test_readme_demo[task2] ____________________________

converter = <model_converter_tool.converter.ModelConverter object at 0x104091d30>
output_dir = PosixPath('test_outputs/basic_conversions')
task = {'input_model': 'gpt2', 'model_type': 'text-generation', 'output_file': 'gpt2.mlx', 'output_format': 'mlx'}

    @pytest.mark.parametrize("task", DEMO_TASKS)
    def test_readme_demo(converter, output_dir, task):
        # If it's an MLX task and not Apple Silicon, automatically skip
        if task["output_format"] == "mlx" and (platform.system() != "Darwin" or platform.machine() != "arm64"):
            pytest.skip("MLX only supported on Apple Silicon macOS")
        # GGUF conversion requires llama-cpp-python and a supported model family
        if task["output_format"] == "gguf":
            if not is_hf_model_available(task["input_model"]):
                pytest.skip(f"Model {task['input_model']} not available on HuggingFace")
        output_path = str(output_dir / task["output_file"])
        result = converter.convert(
            model_name=task["input_model"],
            output_format=task["output_format"],
            output_path=output_path,
            model_type=task["model_type"],
            device="cpu",
        )
>       assert result.success, f"{task['output_format']} conversion failed: {result.error}"
E       AssertionError: mlx conversion failed: Conversion failed for mlx
E       assert False
E        +  where False = ConversionResult(success=False, error='Conversion failed for mlx', output_path=None, validation=None, extra_info=None).success

tests/test_basic_conversions.py:78: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    model_converter_tool.engine.mlx:mlx.py:72 MLX conversion requires mlx. You are on Apple Silicon (macOS arm64). For best performance, install MLX: pip install mlx
=============================== warnings summary ===============================
tests/test_basic_conversions.py::test_readme_demo[task0]
  /Users/duoyun/Desktop/ModelConverterTool/venv/lib/python3.13/site-packages/torch/onnx/utils.py:185: DeprecationWarning: The feature will be removed. Please remove usage of this function
    setup_onnx_logging(verbose) as log_ctx,

tests/test_basic_conversions.py::test_readme_demo[task0]
tests/test_basic_conversions.py::test_readme_demo[task4]
  /Users/duoyun/Desktop/ModelConverterTool/venv/lib/python3.13/site-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
    inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask

tests/test_basic_conversions.py::test_readme_demo[task4]
  /Users/duoyun/Desktop/ModelConverterTool/venv/lib/python3.13/site-packages/torch/jit/annotations.py:388: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_basic_conversions.py::test_readme_demo[task2] - AssertionError: mlx conversion failed: Conversion failed for mlx
assert False
 +  where False = ConversionResult(success=False, error='Conversion failed for mlx', output_path=None, validation=None, extra_info=None).success
============= 1 failed, 5 passed, 1 skipped, 4 warnings in 14.74s ==============
