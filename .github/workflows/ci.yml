name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

# Global environment variables to disable MPS
env:
  CUDA_VISIBLE_DEVICES: ""
  MPS_VISIBLE_DEVICES: ""
  TRANSFORMERS_NO_MPS: "1"
  PYTORCH_ENABLE_MPS_FALLBACK: "1"
  PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
  PYTORCH_MPS_LOW_WATERMARK_RATIO: "0.0"
  USE_CPU_ONLY: "1"
  PYTORCH_NO_CUDA_MEMORY_CACHING: "1"
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:0"

jobs:
  lint-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Upgrade pip and install build tools
        run: |
          python -m pip install --upgrade pip setuptools wheel build

      - name: Install core dependencies
        run: |
          pip install torch>=2.0.0 transformers>=4.30.0 tokenizers>=0.13.0 accelerate>=0.20.0

      - name: Install remaining dependencies
        run: |
          pip install -r requirements.txt

      - name: Install quantization libraries (optional)
        run: |
          pip install datasets>=2.0.0 || echo "datasets installation failed, continuing..."
          pip install logbar || echo "logbar installation failed, continuing..."
          pip install gptqmodel>=0.1.0 || echo "gptqmodel installation failed, continuing..."

      - name: Install dev tools
        run: |
          pip install flake8 black isort pytest pytest-cov

      - name: Build and install package
        run: |
          python -m build --wheel
          pip install dist/*.whl

      - name: Run fast tests
        run: pytest -m fast -v

      - name: Run basic conversion tests
        run: pytest -m basic -v

      - name: Run API usage tests
        run: pytest -m api -v

      - name: Run all tests with coverage
        run: pytest --cov=model_converter_tool --cov-report=term-missing

  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ['3.11']
        exclude:
          - os: macos-latest
            python-version: '3.9'

    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-v2-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-v2-
          ${{ runner.os }}-pip-${{ matrix.python-version }}-

    - name: Install all dependencies first
      run: |
        pip install --no-cache-dir torch==2.7.1
        pip install --no-cache-dir -r requirements.txt
        pip install --no-cache-dir gptqmodel==2.2.0
        pip install --no-cache-dir awqmodel
        pip install --no-cache-dir optimum[gptq]
        pip install --no-cache-dir auto-gptq
        pip install --no-cache-dir auto-awq

    - name: Build and install package
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        python -m build --wheel
        pip install dist/*.whl
        python -c "from model_converter_tool.converter import ModelConverter; print('✅ Package installed successfully')"

    - name: Test CLI help
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        model-converter --help

    - name: Test fast conversions
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        pytest -m fast -v

    - name: Test basic conversions
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        pytest -m basic -v --tb=short --timeout=300

    - name: Test API usage
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        pytest -m api -v --tb=short --timeout=300

    - name: Debug environment and dependencies
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        python -c "import torch; print('torch:', torch.__version__); print('cuda:', torch.cuda.is_available()); print('mps available:', getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available()); print('mps built:', getattr(torch.backends, 'mps', None) and torch.backends.mps.is_built())"
        python -c "import transformers; print('transformers:', transformers.__version__)"
        python -c "import optimum; print('optimum:', optimum.__version__)" || echo "optimum not available"
        python -c "import gptqmodel; print('gptqmodel:', gptqmodel.__version__)" || echo "gptqmodel not available"
        python -c "import awqmodel; print('awqmodel available')" || echo "awqmodel not available"
        python -c "import auto_gptq; print('auto-gptq available')" || echo "auto-gptq not available"
        python -c "import auto_awq; print('auto-awq available')" || echo "auto-awq not available"

    - name: Test simple conversion manually
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        python -c "
        from model_converter_tool import ModelConverter
        import os
        
        converter = ModelConverter()
        print('Testing GPTQ conversion...')
        
        result = converter.convert(
            input_source='gpt2',
            output_format='gptq',
            output_path='./test_gptq_ci',
            model_type='text-generation',
            device='cpu',
            validate=False
        )
        
        print(f'Success: {result[\"success\"]}')
        print(f'Error: {result.get(\"error\")}')
        
        if result['success']:
            print('✅ GPTQ conversion succeeded in CI')
        else:
            print('❌ GPTQ conversion failed in CI')
        "

    - name: Test slow conversions (quantization)
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        # Run quantization tests on all platforms
        pytest -v tests/test_quantization.py -k 'test_gptq_quantization or test_awq_quantization or test_cli_equivalent_quantization' --maxfail=2

    - name: Test batch conversion
      env:
        PYTORCH_ENABLE_MPS_FALLBACK: "1"
        PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"
        TRANSFORMERS_NO_MPS: "1"
        CUDA_VISIBLE_DEVICES: ""
        MPS_VISIBLE_DEVICES: ""
        USE_CPU_ONLY: "1"
      run: |
        python -c "
        from model_converter_tool.converter import ModelConverter
        converter = ModelConverter()
        
        tasks = [
            {
                'input_source': 'gpt2',
                'output_format': 'hf',
                'output_path': 'outputs/batch_test_gpt2_hf',
                'model_type': 'text-generation',
                'device': 'cpu'
            },
            {
                'input_source': 'gpt2',
                'output_format': 'onnx',
                'output_path': 'outputs/batch_test_gpt2_onnx',
                'model_type': 'text-generation',
                'device': 'cpu'
            }
        ]
        
        results = converter.batch_convert(tasks, max_workers=2)
        success_count = sum(1 for r in results if r.get('success'))
        print(f'✅ Batch conversion: {success_count}/{len(results)} successful')
        "

    - name: Upload test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-outputs-${{ matrix.os }}-${{ matrix.python-version }}
        path: outputs/
        retention-days: 1

  integration:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel build
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Build and install package
      run: |
        python -m build --wheel
        pip install dist/*.whl

    - name: Test integration scenarios
      run: |
        python -c "
        from model_converter_tool.converter import ModelConverter
        import os
        
        converter = ModelConverter()
        
        # Test supported formats
        formats = converter.get_supported_formats()
        print(f'✅ Supported formats: {list(formats.keys())}')
        
        # Test conversion info
        info = converter.get_conversion_info('gpt2', 'hf')
        print(f'✅ Conversion info: {info.get(\"supported\", False)}')
        
        # Test validation
        valid = converter.validate_conversion('gpt2', 'hf')
        print(f'✅ Validation: {valid}')
        
        # Test model size estimation
        size = converter._estimate_model_size('gpt2')
        print(f'✅ Model size estimation: {size:.2f} MB')
        "

    - name: Test error handling
      run: |
        python -c "
        from model_converter_tool.converter import ModelConverter
        
        converter = ModelConverter()
        
        # Test invalid model
        result = converter.convert(
            input_source='invalid-model-name',
            output_format='hf',
            output_path='outputs/invalid_test',
            model_type='text-generation',
            device='cpu'
        )
        print(f'✅ Error handling: {not result.get(\"success\", True)}')
        "

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety

    - name: Run security scan
      run: |
        bandit -r model_converter_tool/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 7

  critical-test:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel build
        pip cache purge || echo "Cache purge failed, continuing..."
        pip install -r requirements.txt
        pip install pytest pytest-cov
        # Install quantization libraries separately after torch is installed
        pip install datasets>=2.0.0 || echo "datasets installation failed, continuing..."
        pip install logbar || echo "logbar installation failed, continuing..."
        pip install gptqmodel>=0.1.0 || echo "gptqmodel installation failed, continuing..."

    - name: Build and install package
      run: |
        python -m build --wheel
        pip install dist/*.whl

    - name: Test critical functionality
      run: |
        python -c "from model_converter_tool.converter import ModelConverter; print('✅ Package installed successfully')"
        pytest -m fast -v

  quantization-test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel build
          pip install -r requirements.txt
          pip install datasets logbar gptqmodel device_smi tokenicer threadpoolctl

      - name: Build and install package
        run: |
          python -m build --wheel
          pip install dist/*.whl

      - name: Run quantization-only tests
        env:
          CUDA_VISIBLE_DEVICES: ""
          MPS_VISIBLE_DEVICES: ""
          TRANSFORMERS_NO_MPS: "1"
        run: |
          pytest -m quantization -v

      - name: Show Python version and environment
        run: |
          python --version
          which python
          echo $PATH

      - name: List installed pip packages
        run: |
          pip list

      - name: Check pip dependency health
        run: |
          pip check

      - name: Check quantization dependencies
        run: |
          python check_quant_deps.py || true 