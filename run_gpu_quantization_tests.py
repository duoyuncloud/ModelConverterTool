#!/usr/bin/env python3
"""
GPUæœåŠ¡å™¨é‡åŒ–æµ‹è¯•è„šæœ¬
åŸºäºŽçŽ°æœ‰çš„test_quantization.pyï¼Œä½†ç§»é™¤CPUé™åˆ¶ï¼Œå¯ç”¨GPUåŠ é€Ÿ
"""

import os
import sys
import time
import torch
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUæœåŠ¡å™¨çŽ¯å¢ƒè®¾ç½® - ç§»é™¤CPUé™åˆ¶ï¼Œå¯ç”¨GPU
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# ç§»é™¤è¿™äº›é™åˆ¶ï¼Œè®©GPUå¯ç”¨
# os.environ["CUDA_VISIBLE_DEVICES"] = ""
# os.environ["USE_CPU_ONLY"] = "1"

def check_gpu_environment():
    """æ£€æŸ¥GPUçŽ¯å¢ƒ"""
    logger.info("=== GPUçŽ¯å¢ƒæ£€æŸ¥ ===")
    
    cuda_available = torch.cuda.is_available()
    mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    
    if cuda_available:
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… CUDAå¯ç”¨: {device_name}")
        logger.info(f"   GPUå†…å­˜: {memory_gb:.2f} GB")
        logger.info(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        return "cuda"
    elif mps_available:
        logger.info("âœ… MPSå¯ç”¨")
        return "mps"
    else:
        logger.info("âŒ ä»…CPUå¯ç”¨")
        return "cpu"

def run_quantization_tests():
    """è¿è¡Œé‡åŒ–æµ‹è¯•"""
    logger.info("=== å¼€å§‹GPUé‡åŒ–æµ‹è¯• ===")
    
    from model_converter_tool.converter import ModelConverter
    converter = ModelConverter()
    
    # æµ‹è¯•é…ç½®
    test_models = [
        "facebook/opt-125m",  # å°æ¨¡åž‹ï¼Œå¿«é€Ÿæµ‹è¯•
        "sshleifer/tiny-gpt2"  # æ›´å°çš„æ¨¡åž‹
    ]
    
    output_dir = Path("test_outputs/gpu_quantization")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    test_results = {}
    
    # æµ‹è¯•GPTQé‡åŒ–
    logger.info("æµ‹è¯•GPTQé‡åŒ–...")
    start_time = time.time()
    
    result = converter.convert(
        input_source="facebook/opt-125m",
        output_format="gptq",
        output_path=str(output_dir / "opt_125m_gptq"),
        model_type="text-generation",
        device="auto",  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
        validate=True,
    )
    
    gptq_duration = time.time() - start_time
    test_results["gptq"] = {
        "success": result["success"],
        "duration": gptq_duration,
        "error": result.get("error"),
        "validation": result.get("model_validation", {})
    }
    
    if result["success"]:
        logger.info(f"âœ… GPTQé‡åŒ–æˆåŠŸï¼Œè€—æ—¶: {gptq_duration:.2f}ç§’")
    else:
        logger.error(f"âŒ GPTQé‡åŒ–å¤±è´¥: {result.get('error')}")
    
    # æµ‹è¯•AWQé‡åŒ–
    logger.info("æµ‹è¯•AWQé‡åŒ–...")
    start_time = time.time()
    
    result = converter.convert(
        input_source="facebook/opt-125m",
        output_format="awq",
        output_path=str(output_dir / "opt_125m_awq"),
        model_type="text-generation",
        device="auto",
        validate=True,
    )
    
    awq_duration = time.time() - start_time
    test_results["awq"] = {
        "success": result["success"],
        "duration": awq_duration,
        "error": result.get("error"),
        "validation": result.get("model_validation", {})
    }
    
    if result["success"]:
        logger.info(f"âœ… AWQé‡åŒ–æˆåŠŸï¼Œè€—æ—¶: {awq_duration:.2f}ç§’")
    else:
        logger.error(f"âŒ AWQé‡åŒ–å¤±è´¥: {result.get('error')}")
    
    # æµ‹è¯•GGUFé‡åŒ–
    logger.info("æµ‹è¯•GGUFé‡åŒ–...")
    quantization_levels = ["q4_k_m", "q8_0", "q5_k_m"]
    gguf_results = {}
    
    for quant_level in quantization_levels:
        logger.info(f"  æµ‹è¯•GGUFé‡åŒ–çº§åˆ«: {quant_level}")
        start_time = time.time()
        
        result = converter.convert(
            input_source="sshleifer/tiny-gpt2",
            output_format="gguf",
            output_path=str(output_dir / f"tiny_gpt2_gguf_{quant_level}.gguf"),
            model_type="text-generation",
            quantization=quant_level,
            device="auto",
            validate=True,
        )
        
        duration = time.time() - start_time
        gguf_results[quant_level] = {
            "success": result["success"],
            "duration": duration,
            "error": result.get("error"),
            "validation": result.get("model_validation", {})
        }
        
        if result["success"]:
            logger.info(f"    âœ… GGUF {quant_level} æˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’")
        else:
            logger.error(f"    âŒ GGUF {quant_level} å¤±è´¥: {result.get('error')}")
    
    test_results["gguf"] = gguf_results
    
    return test_results

def analyze_results(test_results):
    """åˆ†æžæµ‹è¯•ç»“æžœ"""
    logger.info("=== æµ‹è¯•ç»“æžœåˆ†æž ===")
    
    # æ€»ä½“ç»Ÿè®¡
    total_tests = 2 + len(test_results["gguf"])  # GPTQ + AWQ + GGUF
    successful_tests = 0
    
    # GPTQç»“æžœ
    gptq_result = test_results["gptq"]
    if gptq_result["success"]:
        successful_tests += 1
        logger.info(f"GPTQé‡åŒ–: âœ… æˆåŠŸ ({gptq_result['duration']:.2f}ç§’)")
    else:
        logger.error(f"GPTQé‡åŒ–: âŒ å¤±è´¥ - {gptq_result['error']}")
    
    # AWQç»“æžœ
    awq_result = test_results["awq"]
    if awq_result["success"]:
        successful_tests += 1
        logger.info(f"AWQé‡åŒ–: âœ… æˆåŠŸ ({awq_result['duration']:.2f}ç§’)")
    else:
        logger.error(f"AWQé‡åŒ–: âŒ å¤±è´¥ - {awq_result['error']}")
    
    # GGUFç»“æžœ
    logger.info("GGUFé‡åŒ–ç»“æžœ:")
    for level, result in test_results["gguf"].items():
        if result["success"]:
            successful_tests += 1
            logger.info(f"  {level}: âœ… æˆåŠŸ ({result['duration']:.2f}ç§’)")
        else:
            logger.error(f"  {level}: âŒ å¤±è´¥ - {result['error']}")
    
    # æˆåŠŸçŽ‡
    success_rate = successful_tests / total_tests * 100
    logger.info(f"æ€»ä½“æˆåŠŸçŽ‡: {success_rate:.1f}% ({successful_tests}/{total_tests})")
    
    if success_rate >= 80:
        logger.info("ðŸŽ‰ GPUé‡åŒ–æµ‹è¯•æ€»ä½“æˆåŠŸï¼")
    elif success_rate >= 50:
        logger.info("âš ï¸ GPUé‡åŒ–æµ‹è¯•éƒ¨åˆ†æˆåŠŸ")
    else:
        logger.error("âŒ GPUé‡åŒ–æµ‹è¯•å¤±è´¥è¾ƒå¤š")
    
    return success_rate

def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹GPUæœåŠ¡å™¨é‡åŒ–æµ‹è¯•")
    
    # 1. æ£€æŸ¥GPUçŽ¯å¢ƒ
    device = check_gpu_environment()
    
    # 2. è¿è¡Œé‡åŒ–æµ‹è¯•
    test_results = run_quantization_tests()
    
    # 3. åˆ†æžç»“æžœ
    success_rate = analyze_results(test_results)
    
    # 4. ä¿å­˜ç»“æžœ
    import json
    with open("gpu_quantization_test_results.json", "w", encoding="utf-8") as f:
        # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_results = {}
        for key, value in test_results.items():
            if isinstance(value, dict):
                serializable_results[key] = {}
                for k, v in value.items():
                    if isinstance(v, (str, int, float, bool, list)):
                        serializable_results[key][k] = v
                    elif isinstance(v, dict):
                        serializable_results[key][k] = {}
                        for k2, v2 in v.items():
                            if isinstance(v2, (str, int, float, bool, list)):
                                serializable_results[key][k][k2] = v2
            elif isinstance(value, (str, int, float, bool)):
                serializable_results[key] = value
        
        serializable_results["device"] = device
        serializable_results["success_rate"] = success_rate
        
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    logger.info("æµ‹è¯•å®Œæˆï¼Œç»“æžœå·²ä¿å­˜åˆ° gpu_quantization_test_results.json")
    
    # è¿”å›žé€€å‡ºç 
    return 0 if success_rate >= 50 else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 