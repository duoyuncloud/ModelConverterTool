# ModelConverterTool

A powerful, flexible tool for converting machine learning models between different formats. Supports text generation, classification, vision, and audio models with comprehensive format coverage.

## ğŸš€ Features

- **Multi-Format Support**: ONNX, GGUF, MLX, TorchScript, FP16, GPTQ, AWQ, SafeTensors, HuggingFace
- **Quantization**: Built-in GPTQ, AWQ, and GGUF quantization support
- **Batch Processing**: Convert multiple models using YAML configuration
- **Cross-Platform**: CPU and GPU (CUDA/MPS) with automatic device detection
- **Validation**: Built-in model validation and compatibility checking
- **API & CLI**: Both programmatic and command-line interfaces

## ğŸ“¦ Installation

```bash
# Clone and install
git clone https://github.com/duoyuncloud/ModelConverterTool.git
cd ModelConverterTool
pip install -e .
```

> **MLX æ”¯æŒï¼ˆä»…é™ macOS arm64/Apple Siliconï¼‰ï¼š**
> å¦‚éœ€ä½¿ç”¨ MLX ç›¸å…³åŠŸèƒ½ï¼Œè¯·åœ¨ Apple Silicon Mac ä¸Šæ‰‹åŠ¨å®‰è£…ï¼š
> ```bash
> pip install mlx
> ```
> æˆ–ï¼ˆå¦‚æ”¯æŒ extra_requiresï¼‰ï¼š
> ```bash
> pip install .[mlx]
> ```

## ğŸ¯ Quick Start

### 1. Basic Model Format Conversion

```bash
# bert-base-uncased â†’ onnx
model-converter convert bert-base-uncased onnx --output-path ./outputs/bert.onnx

# gpt2 â†’ gguf
model-converter convert gpt2 gguf --output-path ./outputs/gpt2.gguf

# gpt2 â†’ mlx
model-converter convert gpt2 mlx --output-path ./outputs/gpt2.mlx

# tiny-gpt2 â†’ fp16
model-converter convert sshleifer/tiny-gpt2 fp16 --output-path ./outputs/tiny_gpt2_fp16

# gpt2 â†’ torchscript
model-converter convert gpt2 torchscript --output-path ./outputs/gpt2.pt

# gpt2 â†’ safetensors
model-converter convert gpt2 safetensors --output-path ./outputs/gpt2_safetensors

# gpt2 â†’ hf (HuggingFace format)
model-converter convert gpt2 hf --output-path ./outputs/gpt2_hf
```

### 2. Quantization

Using `facebook/opt-125m` for quick testing:

```bash
# GPTQ quantization (quick test, é»˜è®¤å°æ ¡å‡†é›†)
model-converter convert facebook/opt-125m gptq --output-path ./outputs/opt_125m_gptq

# AWQ quantization (quick test, é»˜è®¤å°æ ¡å‡†é›†)
model-converter convert facebook/opt-125m awq --output-path ./outputs/opt_125m_awq

# GGUF with quantization
model-converter convert facebook/opt-125m gguf --output-path ./outputs/opt_125m_q4.gguf
```

#### High-Precision Quantization (CLI)

For best quantization quality, you can enable automatic large calibration dataset (256 samples from WikiText-103, each >256 tokens) via CLI:

```bash
# GPTQ high-precision
model-converter convert facebook/opt-125m gptq --output-path ./outputs/opt_125m_gptq --use-large-calibration
# AWQ high-precision
model-converter convert facebook/opt-125m awq --output-path ./outputs/opt_125m_awq --use-large-calibration
```

- `--use-large-calibration` will automatically use a large, high-quality calibration dataset for quantization (slower, recommended for production or final model export).

### 3. Batch Conversion

Create a YAML configuration file:

```yaml
# configs/gpt2_batch.yaml
models:
  gpt2_to_onnx:
    input: "gpt2"
    output_format: "onnx"
    output_path: "outputs/gpt2.onnx"
    model_type: "text-generation"
    device: "cpu"
  
  gpt2_to_gguf:
    input: "gpt2"
    output_format: "gguf"
    output_path: "outputs/gpt2.gguf"
    model_type: "text-generation"
    device: "cpu"
  
  gpt2_to_fp16:
    input: "gpt2"
    output_format: "fp16"
    output_path: "outputs/gpt2_fp16"
    model_type: "text-generation"
    device: "cpu"
  
  gpt2_to_torchscript:
    input: "gpt2"
    output_format: "torchscript"
    output_path: "outputs/gpt2.pt"
    model_type: "text-generation"
    device: "cpu"
  
  gpt2_to_hf:
    input: "gpt2"
    output_format: "hf"
    output_path: "outputs/gpt2_hf"
    model_type: "text-generation"
    device: "cpu"
```

Run batch conversion:

```python
from model_converter_tool.converter import ModelConverter

converter = ModelConverter()
converter.batch_convert_from_yaml("configs/gpt2_batch.yaml")
```

### 4. API Usage

#### ModelConverter().convert

```python
from model_converter_tool.converter import ModelConverter

converter = ModelConverter()

# Basic conversion
result = converter.convert(
    input_source="gpt2",
    output_format="onnx",
    output_path="./outputs/gpt2.onnx",
    model_type="text-generation",
    device="cpu",
    validate=True
)

# All formats
formats = ["onnx", "gguf", "fp16", "torchscript", "hf"]
for fmt in formats:
    result = converter.convert(
        input_source="gpt2",
        output_format=fmt,
        output_path=f"./outputs/gpt2.{fmt}",
        model_type="text-generation",
        device="cpu",
        validate=True
    )

# --- Quantization Examples ---
# Quick quantization (default small calibration set, for testing)
result = converter.convert(
    input_source="facebook/opt-125m",
    output_format="gptq",
    output_path="outputs/opt_125m_gptq_quantized",
    model_type="text-generation",
    device="auto",
    validate=True,
    quantization_config={
        "damp_percent": 0.015,
        # No calibration_dataset: uses small built-in set
    }
)

# AWQ quick quantization
result = converter.convert(
    input_source="facebook/opt-125m",
    output_format="awq",
    output_path="outputs/opt_125m_awq_quantized",
    model_type="text-generation",
    device="auto",
    validate=True,
    quantization_config={
        "damp_percent": 0.015,
    }
)

# High-precision quantization (auto large calibration set, for production)
result = converter.convert(
    input_source="facebook/opt-125m",
    output_format="gptq",
    output_path="outputs/opt_125m_gptq_quantized",
    model_type="text-generation",
    device="auto",
    validate=True,
    quantization_config={
        "damp_percent": 0.015,
    },
    use_large_calibration=True,  # Enable large calibration dataset
)

# AWQ high-precision quantization
result = converter.convert(
    input_source="facebook/opt-125m",
    output_format="awq",
    output_path="outputs/opt_125m_awq_quantized",
    model_type="text-generation",
    device="auto",
    validate=True,
    quantization_config={
        "damp_percent": 0.015,
    },
    use_large_calibration=True,
)
```

#### ModelConverter().batch_convert

```python
# Batch conversion with gpt2
tasks = [
    {
        "input_source": "gpt2",
        "output_format": "onnx",
        "output_path": "./outputs/batch_gpt2.onnx",
        "model_type": "text-generation",
        "device": "cpu"
    },
    {
        "input_source": "gpt2",
        "output_format": "gguf",
        "output_path": "./outputs/batch_gpt2.gguf",
        "model_type": "text-generation",
        "device": "cpu"
    },
    {
        "input_source": "gpt2",
        "output_format": "fp16",
        "output_path": "./outputs/batch_gpt2_fp16",
        "model_type": "text-generation",
        "device": "cpu"
    },
    {
        "input_source": "gpt2",
        "output_format": "torchscript",
        "output_path": "./outputs/batch_gpt2.pt",
        "model_type": "text-generation",
        "device": "cpu"
    },
    {
        "input_source": "gpt2",
        "output_format": "hf",
        "output_path": "./outputs/batch_gpt2_hf",
        "model_type": "text-generation",
        "device": "cpu"
    }
]

results = converter.batch_convert(tasks, max_workers=2)
```

## å®‰è£…ä¸ä½¿ç”¨å»ºè®®

- æ¨èä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…æœ¬å·¥å…·åŒ…ï¼š

```sh
pip install .
# æˆ–
pip install model-converter-tool
```

- å®‰è£…åå¯ç›´æ¥åœ¨å‘½ä»¤è¡Œä½¿ç”¨ `model-converter` æˆ–åœ¨ Python ä¸­ `import model_converter_tool`ã€‚

- **ä¸å»ºè®®**ç›´æ¥è¿è¡Œæºç ç›®å½•ä¸‹çš„æµ‹è¯•è„šæœ¬ï¼ˆå¦‚ `python tests/test_xxx.py`ï¼‰ï¼Œå¦åˆ™å¯èƒ½é‡åˆ° `ModuleNotFoundError: No module named 'model_converter_tool'`ã€‚

- å¦‚é‡ import é”™è¯¯ï¼Œå¯å…ˆæ‰§è¡Œ `pip install .`ï¼Œæˆ–åœ¨æµ‹è¯•è„šæœ¬å¼€å¤´åŠ ï¼š

```python
import sys, os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
```

## å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰

### Q: Windows ä¸‹æç¤º 'model-converter' ä¸æ˜¯å†…éƒ¨æˆ–å¤–éƒ¨å‘½ä»¤ï¼Ÿ
A: å¯èƒ½æ˜¯ Python çš„ Scripts ç›®å½•æœªåŠ å…¥ PATHã€‚å¯ç”¨å¦‚ä¸‹å‘½ä»¤ä»£æ›¿ï¼š

```sh
python -m model_converter_tool.cli [å‚æ•°]
```

å…¶å®ƒå¹³å°å¦‚é‡ç±»ä¼¼é—®é¢˜ä¹Ÿå¯ç”¨æ­¤æ–¹æ³•ã€‚