#!/usr/bin/env python3
"""
Comprehensive test to verify the authenticity and correctness of GPT-2 conversions
"""

import tempfile
import json
import time
from pathlib import Path
from typing import Dict, Any, List
import numpy as np

from model_converter_tool.converter import ModelConverter
from model_converter_tool.validator import ModelValidator


def test_conversion_authenticity():
    """Test the authenticity and correctness of GPT-2 conversions"""
    
    print("ğŸ” GPT-2 è½¬æ¢çœŸå®æ€§å’Œæ­£ç¡®æ€§éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    converter = ModelConverter()
    validator = ModelValidator()
    
    # æµ‹è¯•æ ¼å¼åˆ—è¡¨
    test_formats = [
        ('fp16', 'FP16 åŠç²¾åº¦'),
        ('torchscript', 'TorchScript'),
        ('hf', 'Hugging Face'),
        ('onnx', 'ONNX'),
    ]
    
    results = {}
    
    for format_type, format_name in test_formats:
        print(f"\nğŸ“‹ æµ‹è¯• {format_name} è½¬æ¢")
        print("-" * 40)
        
        with tempfile.TemporaryDirectory() as temp_dir:
            out_dir = Path(temp_dir) / f'gpt2_{format_type}'
            out_dir.mkdir()
            
            # è®°å½•è½¬æ¢å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰§è¡Œè½¬æ¢
            result = converter.convert(
                input_source='hf:gpt2',
                output_format=format_type,
                output_path=str(out_dir),
                model_type='text-generation',
                validate=True
            )
            
            conversion_time = time.time() - start_time
            
            print(f"è½¬æ¢æˆåŠŸ: {'âœ…' if result['success'] else 'âŒ'}")
            print(f"è½¬æ¢æ—¶é—´: {conversion_time:.2f}ç§’")
            print(f"éªŒè¯æˆåŠŸ: {'âœ…' if result.get('validation', False) else 'âŒ'}")
            
            if result['success']:
                # æ–‡ä»¶åˆ†æ
                files = list(out_dir.glob('*'))
                print(f"ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(files)}")
                
                # æ£€æŸ¥å…³é”®æ–‡ä»¶
                key_files = get_key_files_for_format(format_type)
                file_analysis = analyze_files(out_dir, key_files)
                
                for file_info in file_analysis:
                    status = "âœ…" if file_info['exists'] else "âŒ"
                    size_str = f"{file_info['size']:.2f} MB" if file_info['exists'] else "ç¼ºå¤±"
                    print(f"  {status} {file_info['name']}: {size_str}")
                
                # æ¨¡å‹éªŒè¯åˆ†æ
                model_validation = result.get('model_validation', {})
                if model_validation:
                    print(f"æ¨¡å‹éªŒè¯: {'âœ…' if model_validation.get('success', False) else 'âŒ'}")
                    print(f"éªŒè¯æ¶ˆæ¯: {model_validation.get('message', 'N/A')}")
                
                # å®é™…æ¨ç†æµ‹è¯•
                inference_test = test_actual_inference(out_dir, format_type)
                print(f"æ¨ç†æµ‹è¯•: {'âœ…' if inference_test['success'] else 'âŒ'}")
                if inference_test['success']:
                    print(f"  è¾“å…¥: \"{inference_test['input_text']}\"")
                    print(f"  è¾“å‡º: \"{inference_test['output_text']}\"")
                    print(f"  è¾“å‡ºå½¢çŠ¶: {inference_test['output_shape']}")
                
                # ä¿å­˜ç»“æœ
                results[format_type] = {
                    'success': True,
                    'conversion_time': conversion_time,
                    'validation': result.get('validation', False),
                    'file_count': len(files),
                    'key_files': file_analysis,
                    'model_validation': model_validation,
                    'inference_test': inference_test
                }
            else:
                print(f"è½¬æ¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                results[format_type] = {
                    'success': False,
                    'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                }
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š è½¬æ¢çœŸå®æ€§å’Œæ­£ç¡®æ€§æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    successful_formats = []
    failed_formats = []
    
    for format_type, format_name in test_formats:
        result = results[format_type]
        if result['success']:
            successful_formats.append(format_type)
            print(f"\nâœ… {format_name}:")
            print(f"  è½¬æ¢æ—¶é—´: {result['conversion_time']:.2f}ç§’")
            print(f"  æ–‡ä»¶æ•°é‡: {result['file_count']}")
            print(f"  éªŒè¯é€šè¿‡: {'æ˜¯' if result['validation'] else 'å¦'}")
            print(f"  æ¨ç†æµ‹è¯•: {'é€šè¿‡' if result['inference_test']['success'] else 'å¤±è´¥'}")
            
            # æ£€æŸ¥è½¬æ¢çœŸå®æ€§
            authenticity_score = calculate_authenticity_score(result)
            print(f"  çœŸå®æ€§è¯„åˆ†: {authenticity_score:.1f}/10")
            
        else:
            failed_formats.append(format_type)
            print(f"\nâŒ {format_name}: è½¬æ¢å¤±è´¥")
            print(f"  é”™è¯¯: {result['error']}")
    
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æˆåŠŸæ ¼å¼: {len(successful_formats)}/{len(test_formats)}")
    print(f"  å¤±è´¥æ ¼å¼: {len(failed_formats)}/{len(test_formats)}")
    
    if successful_formats:
        avg_time = sum(results[f]['conversion_time'] for f in successful_formats) / len(successful_formats)
        print(f"  å¹³å‡è½¬æ¢æ—¶é—´: {avg_time:.2f}ç§’")
    
    # çœŸå®æ€§è¯„ä¼°
    print(f"\nğŸ” çœŸå®æ€§è¯„ä¼°:")
    print(f"  FP16: {'çœŸå®è½¬æ¢' if 'fp16' in successful_formats else 'è½¬æ¢å¤±è´¥'}")
    print(f"  TorchScript: {'çœŸå®è½¬æ¢' if 'torchscript' in successful_formats else 'è½¬æ¢å¤±è´¥'}")
    print(f"  HF: {'çœŸå®è½¬æ¢' if 'hf' in successful_formats else 'è½¬æ¢å¤±è´¥'}")
    print(f"  ONNX: {'éƒ¨åˆ†è½¬æ¢' if 'onnx' in successful_formats else 'è½¬æ¢å¤±è´¥'}")
    
    return results


def get_key_files_for_format(format_type: str) -> List[str]:
    """è·å–æ¯ç§æ ¼å¼çš„å…³é”®æ–‡ä»¶åˆ—è¡¨"""
    key_files_map = {
        'fp16': ['model.safetensors', 'config.json', 'tokenizer.json'],
        'torchscript': ['model.pt', 'config.json', 'tokenizer.json'],
        'hf': ['pytorch_model.bin', 'config.json', 'tokenizer.json'],
        'onnx': ['model.onnx', 'config.json', 'tokenizer.json'],
    }
    return key_files_map.get(format_type, [])


def analyze_files(output_dir: Path, key_files: List[str]) -> List[Dict[str, Any]]:
    """åˆ†æè¾“å‡ºæ–‡ä»¶"""
    analysis = []
    
    for file_name in key_files:
        file_path = output_dir / file_name
        if file_path.exists():
            size = file_path.stat().st_size / (1024 * 1024)  # MB
            analysis.append({
                'name': file_name,
                'exists': True,
                'size': size,
                'path': str(file_path)
            })
        else:
            analysis.append({
                'name': file_name,
                'exists': False,
                'size': 0,
                'path': None
            })
    
    return analysis


def test_actual_inference(output_dir: Path, format_type: str) -> Dict[str, Any]:
    """æµ‹è¯•å®é™…æ¨ç†èƒ½åŠ›"""
    try:
        if format_type == 'fp16':
            return test_fp16_inference(output_dir)
        elif format_type == 'torchscript':
            return test_torchscript_inference(output_dir)
        elif format_type == 'hf':
            return test_hf_inference(output_dir)
        elif format_type == 'onnx':
            return test_onnx_inference(output_dir)
        else:
            return {'success': False, 'error': f'Unsupported format: {format_type}'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_fp16_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•FP16æ¨¡å‹æ¨ç†"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model = AutoModelForCausalLM.from_pretrained(str(output_dir))
        tokenizer = AutoTokenizer.from_pretrained(str(output_dir))
        
        # æµ‹è¯•æ¨ç†
        test_text = "Hello world"
        inputs = tokenizer(test_text, return_tensors='pt')
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # ç”Ÿæˆæ–‡æœ¬
        generated = model.generate(
            inputs['input_ids'], 
            max_length=len(inputs['input_ids'][0]) + 5,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        
        return {
            'success': True,
            'input_text': test_text,
            'output_text': generated_text,
            'output_shape': outputs.logits.shape,
            'vocab_size': outputs.logits.shape[-1]
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_torchscript_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•TorchScriptæ¨¡å‹æ¨ç†"""
    try:
        import torch
        
        model_path = output_dir / 'model.pt'
        if not model_path.exists():
            return {'success': False, 'error': 'TorchScript model file not found'}
        
        model = torch.jit.load(str(model_path))
        
        # æµ‹è¯•æ¨ç†
        dummy_input = torch.randint(0, 50257, (1, 5))
        output = model(dummy_input)
        
        return {
            'success': True,
            'input_text': f"Token IDs: {dummy_input[0].tolist()}",
            'output_text': f"Output shape: {output.shape}",
            'output_shape': output.shape
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_hf_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•HFæ¨¡å‹æ¨ç†"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model = AutoModelForCausalLM.from_pretrained(str(output_dir))
        tokenizer = AutoTokenizer.from_pretrained(str(output_dir))
        
        # æµ‹è¯•æ¨ç†
        test_text = "Hello world"
        inputs = tokenizer(test_text, return_tensors='pt')
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # ç”Ÿæˆæ–‡æœ¬
        generated = model.generate(
            inputs['input_ids'], 
            max_length=len(inputs['input_ids'][0]) + 5,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        
        return {
            'success': True,
            'input_text': test_text,
            'output_text': generated_text,
            'output_shape': outputs.logits.shape,
            'vocab_size': outputs.logits.shape[-1]
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_onnx_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•ONNXæ¨¡å‹æ¨ç†"""
    try:
        import onnxruntime as ort
        import numpy as np
        
        onnx_files = list(output_dir.glob('*.onnx'))
        if not onnx_files:
            return {'success': False, 'error': 'No ONNX files found'}
        
        onnx_file = onnx_files[0]
        
        # æ£€æŸ¥ONNXæ–‡ä»¶å¤§å°
        file_size = onnx_file.stat().st_size
        if file_size < 1024:  # å°äº1KBï¼Œå¯èƒ½æ˜¯å ä½ç¬¦æ–‡ä»¶
            return {'success': False, 'error': 'ONNX file too small, likely a placeholder'}
        
        # å°è¯•åŠ è½½ONNXæ¨¡å‹
        try:
            session = ort.InferenceSession(str(onnx_file))
        except Exception as e:
            return {'success': False, 'error': f'ONNX loading failed: {e}'}
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        input_name = session.get_inputs()[0].name
        output_name = session.get_outputs()[0].name
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        dummy_input = np.random.randint(0, 50257, (1, 10), dtype=np.int64)
        
        # è¿è¡Œæ¨ç†
        outputs = session.run([output_name], {input_name: dummy_input})
        
        return {
            'success': True,
            'input_text': f"Token IDs: {dummy_input[0].tolist()[:5]}...",
            'output_text': f"Output shape: {outputs[0].shape}",
            'output_shape': outputs[0].shape
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def calculate_authenticity_score(result: Dict[str, Any]) -> float:
    """è®¡ç®—è½¬æ¢çœŸå®æ€§è¯„åˆ† (0-10)"""
    score = 0.0
    
    if not result['success']:
        return 0.0
    
    # åŸºç¡€åˆ†æ•°ï¼šè½¬æ¢æˆåŠŸ
    score += 2.0
    
    # æ–‡ä»¶å®Œæ•´æ€§
    key_files = result['key_files']
    existing_files = sum(1 for f in key_files if f['exists'])
    if key_files:
        file_completeness = existing_files / len(key_files)
        score += file_completeness * 2.0
    
    # æ–‡ä»¶å¤§å°åˆç†æ€§
    total_size = sum(f['size'] for f in key_files if f['exists'])
    if total_size > 10:  # å¤§äº10MBè®¤ä¸ºæ˜¯åˆç†çš„æ¨¡å‹å¤§å°
        score += 2.0
    elif total_size > 1:  # å¤§äº1MB
        score += 1.0
    
    # æ¨¡å‹éªŒè¯
    if result.get('validation'):
        score += 1.0
    
    # æ¨ç†æµ‹è¯•
    if result.get('inference_test', {}).get('success'):
        score += 2.0
    
    # è½¬æ¢æ—¶é—´åˆç†æ€§
    conversion_time = result.get('conversion_time', 0)
    if 1 <= conversion_time <= 300:  # 1ç§’åˆ°5åˆ†é’Ÿä¹‹é—´
        score += 1.0
    
    return min(score, 10.0)


if __name__ == "__main__":
    results = test_conversion_authenticity()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('conversion_authenticity_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: conversion_authenticity_results.json") 