#!/usr/bin/env python3
"""
Comprehensive test for optimized conversions including ONNX, TorchScript, and quantization
"""

import tempfile
import json
import time
from pathlib import Path
from typing import Dict, Any, List
import numpy as np

from model_converter_tool.converter import ModelConverter
from model_converter_tool.validator import ModelValidator


def test_optimized_conversions():
    """Test all optimized conversions with enhanced validation"""
    
    print("ğŸš€ ä¼˜åŒ–åè½¬æ¢åŠŸèƒ½å…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    converter = ModelConverter()
    validator = ModelValidator()
    
    # æµ‹è¯•æ ¼å¼åˆ—è¡¨ï¼ˆåŒ…æ‹¬é‡åŒ–æ ¼å¼ï¼‰
    test_formats = [
        ('fp16', 'FP16 åŠç²¾åº¦', None),
        ('torchscript', 'TorchScript', None),
        ('hf', 'Hugging Face', None),
        ('onnx', 'ONNX', None),
        ('gptq', 'GPTQ é‡åŒ–', 'q4_k_m'),
        ('gguf', 'GGUF é‡åŒ–', 'q4_k_m'),
    ]
    
    results = {}
    
    for format_type, format_name, quantization in test_formats:
        print(f"\nğŸ“‹ æµ‹è¯• {format_name} è½¬æ¢")
        print("-" * 50)
        
        with tempfile.TemporaryDirectory() as temp_dir:
            out_dir = Path(temp_dir) / f'gpt2_{format_type}'
            out_dir.mkdir()
            
            # è®°å½•è½¬æ¢å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰§è¡Œè½¬æ¢
            result = converter.convert(
                input_source='hf:gpt2',
                output_format=format_type,
                output_path=str(out_dir),
                model_type='text-generation',
                quantization=quantization,
                validate=True
            )
            
            conversion_time = time.time() - start_time
            
            print(f"è½¬æ¢æˆåŠŸ: {'âœ…' if result['success'] else 'âŒ'}")
            print(f"è½¬æ¢æ—¶é—´: {conversion_time:.2f}ç§’")
            print(f"éªŒè¯æˆåŠŸ: {'âœ…' if result.get('validation', False) else 'âŒ'}")
            
            if result['success']:
                # æ–‡ä»¶åˆ†æ
                files = list(out_dir.glob('*'))
                print(f"ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(files)}")
                
                # æ£€æŸ¥å…³é”®æ–‡ä»¶
                key_files = get_key_files_for_format(format_type)
                file_analysis = analyze_files(out_dir, key_files)
                
                for file_info in file_analysis:
                    status = "âœ…" if file_info['exists'] else "âŒ"
                    size_str = f"{file_info['size']:.2f} MB" if file_info['exists'] else "ç¼ºå¤±"
                    print(f"  {status} {file_info['name']}: {size_str}")
                
                # æ¨¡å‹éªŒè¯åˆ†æ
                model_validation = result.get('model_validation', {})
                if model_validation:
                    print(f"æ¨¡å‹éªŒè¯: {'âœ…' if model_validation.get('success', False) else 'âŒ'}")
                    print(f"éªŒè¯æ¶ˆæ¯: {model_validation.get('message', 'N/A')}")
                    
                    # é‡åŒ–è´¨é‡éªŒè¯
                    quality_validation = model_validation.get('quality_validation')
                    if quality_validation and quality_validation.get('success'):
                        quality_score = quality_validation.get('quality_score', 0)
                        compression_ratio = quality_validation.get('compression_ratio', 1)
                        similarity = quality_validation.get('avg_similarity', 0)
                        print(f"é‡åŒ–è´¨é‡: {quality_score}/10")
                        print(f"å‹ç¼©æ¯”: {compression_ratio:.2f}x")
                        print(f"ç›¸ä¼¼åº¦: {similarity:.3f}")
                
                # å®é™…æ¨ç†æµ‹è¯•
                inference_test = test_actual_inference(out_dir, format_type, quantization)
                print(f"æ¨ç†æµ‹è¯•: {'âœ…' if inference_test['success'] else 'âŒ'}")
                if inference_test['success']:
                    print(f"  è¾“å…¥: \"{inference_test['input_text']}\"")
                    print(f"  è¾“å‡º: \"{inference_test['output_text']}\"")
                    print(f"  è¾“å‡ºå½¢çŠ¶: {inference_test['output_shape']}")
                
                # ä¿å­˜ç»“æœ
                results[format_type] = {
                    'success': True,
                    'conversion_time': conversion_time,
                    'validation': result.get('validation', False),
                    'file_count': len(files),
                    'key_files': file_analysis,
                    'model_validation': model_validation,
                    'inference_test': inference_test,
                    'quantization': quantization
                }
            else:
                print(f"è½¬æ¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                results[format_type] = {
                    'success': False,
                    'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                }
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¼˜åŒ–åè½¬æ¢åŠŸèƒ½æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    successful_formats = []
    failed_formats = []
    
    for format_type, format_name, quantization in test_formats:
        result = results[format_type]
        if result['success']:
            successful_formats.append(format_type)
            print(f"\nâœ… {format_name}:")
            print(f"  è½¬æ¢æ—¶é—´: {result['conversion_time']:.2f}ç§’")
            print(f"  æ–‡ä»¶æ•°é‡: {result['file_count']}")
            print(f"  éªŒè¯é€šè¿‡: {'æ˜¯' if result['validation'] else 'å¦'}")
            print(f"  æ¨ç†æµ‹è¯•: {'é€šè¿‡' if result['inference_test']['success'] else 'å¤±è´¥'}")
            
            # æ£€æŸ¥è½¬æ¢çœŸå®æ€§
            authenticity_score = calculate_authenticity_score(result)
            print(f"  çœŸå®æ€§è¯„åˆ†: {authenticity_score:.1f}/10")
            
            # é‡åŒ–ä¿¡æ¯
            if quantization:
                quality_validation = result.get('model_validation', {}).get('quality_validation')
                if quality_validation and quality_validation.get('success'):
                    quality_score = quality_validation.get('quality_score', 0)
                    compression_ratio = quality_validation.get('compression_ratio', 1)
                    print(f"  é‡åŒ–è´¨é‡: {quality_score}/10")
                    print(f"  å‹ç¼©æ¯”: {compression_ratio:.2f}x")
            
        else:
            failed_formats.append(format_type)
            print(f"\nâŒ {format_name}: è½¬æ¢å¤±è´¥")
            print(f"  é”™è¯¯: {result['error']}")
    
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æˆåŠŸæ ¼å¼: {len(successful_formats)}/{len(test_formats)}")
    print(f"  å¤±è´¥æ ¼å¼: {len(failed_formats)}/{len(test_formats)}")
    
    if successful_formats:
        avg_time = sum(results[f]['conversion_time'] for f in successful_formats) / len(successful_formats)
        print(f"  å¹³å‡è½¬æ¢æ—¶é—´: {avg_time:.2f}ç§’")
    
    # ä¼˜åŒ–æ•ˆæœè¯„ä¼°
    print(f"\nğŸ”§ ä¼˜åŒ–æ•ˆæœè¯„ä¼°:")
    print(f"  ONNX: {'âœ… çœŸå®è½¬æ¢' if 'onnx' in successful_formats else 'âŒ è½¬æ¢å¤±è´¥'}")
    print(f"  TorchScript: {'âœ… çœŸå®è½¬æ¢' if 'torchscript' in successful_formats else 'âŒ è½¬æ¢å¤±è´¥'}")
    print(f"  GPTQ: {'âœ… é‡åŒ–è½¬æ¢' if 'gptq' in successful_formats else 'âŒ è½¬æ¢å¤±è´¥'}")
    print(f"  GGUF: {'âœ… é‡åŒ–è½¬æ¢' if 'gguf' in successful_formats else 'âŒ è½¬æ¢å¤±è´¥'}")
    
    return results


def get_key_files_for_format(format_type: str) -> List[str]:
    """è·å–æ¯ç§æ ¼å¼çš„å…³é”®æ–‡ä»¶åˆ—è¡¨"""
    key_files_map = {
        'fp16': ['model.safetensors', 'config.json', 'tokenizer.json'],
        'torchscript': ['model.pt', 'config.json', 'tokenizer.json'],
        'hf': ['pytorch_model.bin', 'config.json', 'tokenizer.json'],
        'onnx': ['model.onnx', 'config.json', 'tokenizer.json'],
        'gptq': ['model.safetensors', 'config.json', 'tokenizer.json'],
        'gguf': ['model.gguf', 'config.json', 'tokenizer.json'],
    }
    return key_files_map.get(format_type, [])


def analyze_files(output_dir: Path, key_files: List[str]) -> List[Dict[str, Any]]:
    """åˆ†æè¾“å‡ºæ–‡ä»¶"""
    analysis = []
    
    for file_name in key_files:
        file_path = output_dir / file_name
        if file_path.exists():
            size = file_path.stat().st_size / (1024 * 1024)  # MB
            analysis.append({
                'name': file_name,
                'exists': True,
                'size': size,
                'path': str(file_path)
            })
        else:
            analysis.append({
                'name': file_name,
                'exists': False,
                'size': 0,
                'path': None
            })
    
    return analysis


def test_actual_inference(output_dir: Path, format_type: str, quantization: str = None) -> Dict[str, Any]:
    """æµ‹è¯•å®é™…æ¨ç†èƒ½åŠ›"""
    try:
        if format_type == 'fp16':
            return test_fp16_inference(output_dir)
        elif format_type == 'torchscript':
            return test_torchscript_inference(output_dir)
        elif format_type == 'hf':
            return test_hf_inference(output_dir)
        elif format_type == 'onnx':
            return test_onnx_inference(output_dir)
        elif format_type == 'gptq':
            return test_gptq_inference(output_dir)
        elif format_type == 'gguf':
            return test_gguf_inference(output_dir)
        else:
            return {'success': False, 'error': f'Unsupported format: {format_type}'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_fp16_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•FP16æ¨¡å‹æ¨ç†"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model = AutoModelForCausalLM.from_pretrained(str(output_dir))
        tokenizer = AutoTokenizer.from_pretrained(str(output_dir))
        
        # æµ‹è¯•æ¨ç†
        test_text = "Hello world"
        inputs = tokenizer(test_text, return_tensors='pt')
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # ç”Ÿæˆæ–‡æœ¬
        generated = model.generate(
            inputs['input_ids'], 
            max_length=len(inputs['input_ids'][0]) + 5,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        
        return {
            'success': True,
            'input_text': test_text,
            'output_text': generated_text,
            'output_shape': outputs.logits.shape,
            'vocab_size': outputs.logits.shape[-1]
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_torchscript_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•TorchScriptæ¨¡å‹æ¨ç†"""
    try:
        import torch
        
        model_path = output_dir / 'model.pt'
        if not model_path.exists():
            return {'success': False, 'error': 'TorchScript model file not found'}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = model_path.stat().st_size / (1024 * 1024)  # MB
        if file_size < 1:
            return {'success': False, 'error': 'TorchScript file too small, likely a placeholder'}
        
        model = torch.jit.load(str(model_path))
        
        # æµ‹è¯•æ¨ç†
        dummy_input = torch.randint(0, 50257, (1, 8))
        output = model(dummy_input)
        
        return {
            'success': True,
            'input_text': f"Token IDs: {dummy_input[0].tolist()[:5]}...",
            'output_text': f"Output shape: {output.shape}",
            'output_shape': output.shape,
            'file_size_mb': file_size
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_hf_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•HFæ¨¡å‹æ¨ç†"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model = AutoModelForCausalLM.from_pretrained(str(output_dir))
        tokenizer = AutoTokenizer.from_pretrained(str(output_dir))
        
        # æµ‹è¯•æ¨ç†
        test_text = "Hello world"
        inputs = tokenizer(test_text, return_tensors='pt')
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # ç”Ÿæˆæ–‡æœ¬
        generated = model.generate(
            inputs['input_ids'], 
            max_length=len(inputs['input_ids'][0]) + 5,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
        
        return {
            'success': True,
            'input_text': test_text,
            'output_text': generated_text,
            'output_shape': outputs.logits.shape,
            'vocab_size': outputs.logits.shape[-1]
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_onnx_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•ONNXæ¨¡å‹æ¨ç†"""
    try:
        import onnxruntime as ort
        import numpy as np
        
        onnx_files = list(output_dir.glob('*.onnx'))
        if not onnx_files:
            return {'success': False, 'error': 'No ONNX files found'}
        
        onnx_file = onnx_files[0]
        
        # æ£€æŸ¥ONNXæ–‡ä»¶å¤§å°
        file_size = onnx_file.stat().st_size / (1024 * 1024)  # MB
        if file_size < 0.001:  # å°äº1KBï¼Œå¯èƒ½æ˜¯å ä½ç¬¦æ–‡ä»¶
            return {'success': False, 'error': 'ONNX file too small, likely a placeholder'}
        
        # å°è¯•åŠ è½½ONNXæ¨¡å‹
        try:
            session = ort.InferenceSession(str(onnx_file))
        except Exception as e:
            return {'success': False, 'error': f'ONNX loading failed: {e}'}
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        input_name = session.get_inputs()[0].name
        output_name = session.get_outputs()[0].name
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        dummy_input = np.random.randint(0, 50257, (1, 8), dtype=np.int64)
        
        # è¿è¡Œæ¨ç†
        outputs = session.run([output_name], {input_name: dummy_input})
        
        return {
            'success': True,
            'input_text': f"Token IDs: {dummy_input[0].tolist()[:5]}...",
            'output_text': f"Output shape: {outputs[0].shape}",
            'output_shape': outputs[0].shape,
            'file_size_mb': file_size
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_gptq_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•GPTQæ¨¡å‹æ¨ç†"""
    try:
        from auto_gptq import AutoGPTQForCausalLM
        from transformers import AutoTokenizer
        import torch
        
        # åŠ è½½GPTQæ¨¡å‹
        model = AutoGPTQForCausalLM.from_quantized(str(output_dir))
        tokenizer = AutoTokenizer.from_pretrained(str(output_dir))
        
        # æµ‹è¯•æ¨ç†
        test_text = "Hello world"
        inputs = tokenizer(test_text, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # éªŒè¯è¾“å‡º
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
        else:
            logits = outputs
        
        return {
            'success': True,
            'input_text': test_text,
            'output_text': f"GPTQ output shape: {logits.shape}",
            'output_shape': logits.shape,
            'quantization': 'GPTQ'
        }
    except ImportError:
        return {'success': False, 'error': 'auto-gptq not available'}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def test_gguf_inference(output_dir: Path) -> Dict[str, Any]:
    """æµ‹è¯•GGUFæ¨¡å‹æ¨ç†"""
    try:
        model_dir = Path(output_dir)
        gguf_files = list(model_dir.glob('*.gguf'))
        if not gguf_files:
            return {'success': False, 'error': 'No GGUF files found'}
        
        gguf_file = gguf_files[0]
        
        # æ£€æŸ¥GGUFæ–‡ä»¶å¤´
        with open(gguf_file, 'rb') as f:
            header = f.read(8)
            if not header.startswith(b'GGUF'):
                return {'success': False, 'error': 'Invalid GGUF file header'}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = gguf_file.stat().st_size / (1024 * 1024)  # MB
        
        return {
            'success': True,
            'input_text': 'GGUF model loaded',
            'output_text': f'GGUF file size: {file_size:.2f} MB',
            'output_shape': 'GGUF format',
            'file_size_mb': file_size,
            'quantization': 'GGUF'
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}


def calculate_authenticity_score(result: Dict[str, Any]) -> float:
    """è®¡ç®—è½¬æ¢çœŸå®æ€§è¯„åˆ† (0-10)"""
    score = 0.0
    
    if not result['success']:
        return 0.0
    
    # åŸºç¡€åˆ†æ•°ï¼šè½¬æ¢æˆåŠŸ
    score += 2.0
    
    # æ–‡ä»¶å®Œæ•´æ€§
    key_files = result['key_files']
    existing_files = sum(1 for f in key_files if f['exists'])
    if key_files:
        file_completeness = existing_files / len(key_files)
        score += file_completeness * 2.0
    
    # æ–‡ä»¶å¤§å°åˆç†æ€§
    total_size = sum(f['size'] for f in key_files if f['exists'])
    if total_size > 10:  # å¤§äº10MBè®¤ä¸ºæ˜¯åˆç†çš„æ¨¡å‹å¤§å°
        score += 2.0
    elif total_size > 1:  # å¤§äº1MB
        score += 1.0
    
    # æ¨¡å‹éªŒè¯
    if result.get('validation'):
        score += 1.0
    
    # æ¨ç†æµ‹è¯•
    if result.get('inference_test', {}).get('success'):
        score += 2.0
    
    # è½¬æ¢æ—¶é—´åˆç†æ€§
    conversion_time = result.get('conversion_time', 0)
    if 1 <= conversion_time <= 300:  # 1ç§’åˆ°5åˆ†é’Ÿä¹‹é—´
        score += 1.0
    
    return min(score, 10.0)


if __name__ == "__main__":
    results = test_optimized_conversions()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('optimized_conversion_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: optimized_conversion_results.json") 